<html>

<head>
<title>
All publications sorted by year</title>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="keywords" lang="en" content="bibtex2html, bibliography, article, report">
<META name="GENERATOR" content="bibtex2html 1.01">
</head>
<body bgcolor="#ffffff" link="blue" alink="blue" vlink="blue">


<br />
<a href="../index.html"><strong> BACK TO INDEX </strong></a>
<br /><br />


<table width="100%">
<tr><td height="50" bgcolor="#669999" align="center">
<strong><font size=6 color="#ffffff" face="times">
All publications sorted by year
</font></strong>
</td></tr>
</table>


<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2021
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:parfwd:MTJ2021"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Parallel Feature Weight Decay Algorithms for Fast Development of Machine Translation Models</strong>.
<em>Machine Translation</em>,
2021.
<strong>ISSN:</strong> 0922-6567.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Parallel feature weight decay algorithms, parfwd, are engineered for language- and task-adaptive instance selection to build distinct machine translation (MT) models and enable the fast development of accurate MT using fewer data and less computation. parfwd decay the weights of both source and target features to increase their average coverage. In a conference on MT (WMT), parfwd achieved the lowest translation error rate from French to English in 2015, and a rate $11.7\%$ less than the top phrase-based statistical MT (PBSMT) in 2017. parfwd also achieved a rate $5.8\%$ less than the top in TweetMT and the top from Catalan to English. BLEU upper bounds identify the translation directions that offer the largest room for relative improvement and MT models that use additional data. Performance trends angle shows the power of MT models to convert unit data into unit translation results or more BLEU for an increase in coverage. The source coverage angle of parfwd in the 2013--2019 WMT reached +6	extdegree \, better than the top with $35$	extdegree \, for translation into English, and it was +1.4	extdegree \, better than the top with $22$	extdegree \, overall.</td>

</tr></table></center>
[bibtex-key = Bicici:parfwd:MTJ2021]
[<a href="../Year/2021.complete.html#Bicici:parfwd:MTJ2021">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2020
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:RTM:WMT2020"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>RTM Ensemble Learning Results at Quality Estimation Task</strong>.
In <em>Proc. of the Fifth Conf. on Machine Translation (WMT20)</em>,
Online,
11 2020.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking results significantly improve the results on the training sets but decrease the test set results. RTMs can achieve to become the $5$th among $13$ models in ru-en subtask and $5$th in the multilingual track of sentence-level Task 1 based on MAE.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2020]
[<a href="../Year/2020.complete.html#Bicici:RTM:WMT2020">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2019
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:parfda:WMT2019"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Machine Translation with parfda, Moses, kenlm, nplm, and PRO</strong>.
In <em>Proc. of the Fourth Conf. on Machine Translation (WMT19)</em>,
Florence, Italy,
8 2019.
[<a href="http://bicici.github.io/publications/2019/parfda_WMT.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We build 	exttt{parfda} Moses statistical machine translation (SMT) models for most language pairs in the news translation task. We experiment with a hybrid approach using neural language models integrated into Moses. We obtain the constrained data statistics on the machine translation task, the coverage of the test sets, and the upper bounds on the translation results. We also contribute a new testsuite for the German-English language pair and a new automated key phrase extraction technique for the evaluation of the testsuite translations.</td>

</tr></table></center>
[bibtex-key = Bicici:parfda:WMT2019]
[<a href="../Year/2019.complete.html#Bicici:parfda:WMT2019">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:WMT2019"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>RTM Stacking Results for Machine Translation Performance Prediction</strong>.
In <em>Proc. of the Fourth Conf. on Machine Translation (WMT19)</em>,
Florence, Italy,
8 2019.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine features extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2019]
[<a href="../Year/2019.complete.html#Bicici:RTM:WMT2019">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2018
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:RTM:WMT2018"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>RTM results for Predicting Translation Performance</strong>.
In <em>Proc. of the Third Conf. on Machine Translation (WMT18)</em>,
Brussels, Belgium,
pages 765-769,
10 2018.
[<a href="https://aclweb.org/anthology/papers/W/W18/W18-6458/">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
With improved prediction combination using weights based on their training performance and stacking and multilayer perceptrons to build deeper prediction models, RTMs become the 3rd system in general at the sentence-level prediction of translation scores and achieve the lowest RMSE in English to German NMT QET results. For the document-level task, we compare document-level RTM models with sentence-level RTM models obtained with the concatenation of document sentences and obtain similar results.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2018]
[<a href="../Year/2018.complete.html#Bicici:RTM:WMT2018">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:parfda:WMT2018"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Robust parfda Statistical Machine Translation Results</strong>.
In <em>Proc. of the Third Conf. on Machine Translation (WMT18)</em>,
Brussels, Belgium,
pages 345-354,
10 2018.
[<a href="https://aclweb.org/anthology/papers/W/W18/W18-6405/">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We build parallel feature decay algorithms (	exttt{parfda}) Moses statistical machine translation (SMT) models for language pairs in the translation task. 	exttt{parfda} obtains results close to the top constrained phrase-based SMT with an average of $2.252$ BLEU points difference on WMT 2017 datasets using significantly less computation for building SMT systems than that would be spent using all available corpora. We obtain BLEU upper bounds based on target coverage to identify which systems used additional data. We use PRO for tuning to decrease fluctuations in the results and post-process translation outputs to decrease translation errors due to the casing of words. $F_1$ scores on the key phrases of the English to Turkish testsuite that we prepared reveal that 	exttt{parfda} achieves $2nd$ best results. Truecasing translations before scoring obtained the best results overall.</td>

</tr></table></center>
[bibtex-key = Bicici:parfda:WMT2018]
[<a href="../Year/2018.complete.html#Bicici:parfda:WMT2018">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2017
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:CloudMonitor:B3S2017"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Enerji HarcamalarÄ±nÄ± Azaltmak iÃ§in Bulut MonitorÃ¼ (A Cloud Monitor for Reducing Energy Consumption)</strong>.
In <em>First Symposium on Cloud Computing and Big Data (B3S17)</em>,
Antalya, Turkey,
pages 117-122,
10 2017.
TÃœBITAK.
[<a href="http://www.b3s.b3lab.org/">WWW</a>]
[bibtex-key = Bicici:CloudMonitor:B3S2017]
[<a href="../Year/2017.complete.html#Bicici:CloudMonitor:B3S2017">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:WMT2017"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Predicting Translation Performance with Referential Translation Machines</strong>.
In <em>Proc. of the Second Conf. on Machine Translation (WMT17)</em>,
Copenhagen, Denmark,
pages 540-544,
9 2017.
[<a href="http://www.aclweb.org/anthology/W17-4759">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Referential translation machines achieve top performance in both bilingual and monolingual settings without accessing any task or domain specific information or resource. RTMs achieve the $3$rd system results for German to English sentence-level prediction of translation quality and the $2$nd system results according to root mean squared error. In addition to the new features about substring distances, punctuation tokens, character $n$-grams, and alignment crossings, and additional learning models, we average prediction scores from different models using weights based on their training performance for improved results.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2017]
[<a href="../Year/2017.complete.html#Bicici:RTM:WMT2017">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:SEMEVAL2017"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>RTM at SemEval-2017 Task 1: Referential Translation Machines for Predicting Semantic Similarity</strong>.
In <em>11th International Workshop on Semantic Evaluation (SemEval-2017)</em>,
Vancouver, Canada,
pages 194-198,
8 2017.
[<a href="http://nlp.arizona.edu/SemEval-2017/pdf/SemEval030.pdf">PDF</a>]
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use referential translation machines for predicting the semantic similarity of text in all STS tasks which contain Arabic, English, Spanish, and Turkish this year. RTMs pioneer a language independent approach to semantic similarity and remove the need to access any task or domain specific information or resource. RTMs become 6th out of 52 submissions in Spanish to English STS. We average prediction scores using weights based on the training performance to improve the overall performance.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:SEMEVAL2017]
[<a href="../Year/2017.complete.html#Bicici:RTM:SEMEVAL2017">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2016
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:RTMPPP:PBML2016"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Predicting the Performance of Parsing with Referential Translation Machines</strong>.
<em>The Prague Bulletin of Mathematical Linguistics</em>,
106:31-44,
2016.
<strong>ISSN:</strong> 1804-0462.
[doi:<a href="http://dx.doi.org/10.1515/pralin-2016-0010">10.1515/pralin-2016-0010</a>]
Keyword(s): <a href="../Keyword/REFERENTIAL-TRANSLATION-MACHINES.html">referential translation machines</a>,
<a href="../Keyword/MACHINE-TRANSLATION.html">machine translation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Referential translation machine (RTM) is a prediction engine used for predicting the performance of natural language processing tasks including parsing, machine translation, and semantic similarity pioneering language, task, and domain independence. RTM results for predicting the performance of parsing (PPP) in out-of-domain or in-domain settings with different training sets and types of features present results independent of language or parser. RTM PPP models can be used without parsing using only text input and without any parser or language dependent information. Our results detail prediction performance, top selected features, and lower bound on the prediction error of PPP.</td>

</tr></table></center>
[bibtex-key = Bicici:RTMPPP:PBML2016]
[<a href="../Year/2016.complete.html#Bicici:RTMPPP:PBML2016">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:ParFDA:WMT2016"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>ParFDA for Instance Selection for Statistical Machine Translation</strong>.
In <em>Proc. of the First Conf. on Machine Translation (WMT16)</em>,
Berlin, Germany,
pages 252-258,
8 2016.
[<a href="https://aclanthology.info/papers/W16-2306/w16-2306">WWW</a>]
[<a href="http://bicici.github.com/publications/2016/ParFDA_WMT.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/LANGUAGE-MODELING.html">Language Modeling</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We build parallel feature decay algorithms (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the translation task at the first conference on statistical machine translation~\cite{WMT2016} (WMT16). ParFDA obtains results close to the top constrained phrase-based SMT with an average of $2.52$ BLEU points difference using significantly less computation for building SMT systems than the computation that would be spent using all available corpora. We obtain BLEU bounds based on target coverage and show that ParFDA results can be improved by $12.6$ BLEU points on average. Similar bounds show that top constrained SMT results at WMT16 can be improved by $8$ BLEU points on average while German to English and Romanian to English translations results are already close to the bounds.</td>

</tr></table></center>
[bibtex-key = Bicici:ParFDA:WMT2016]
[<a href="../Year/2016.complete.html#Bicici:ParFDA:WMT2016">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:SEMEVAL2016"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>RTM at SemEval-2016 Task 1: Predicting Semantic Similarity with Referential Translation Machines and Related Statistics</strong>.
In <em>SemEval-2016: Semantic Evaluation Exercises - Inter. Workshop on Semantic Evaluation</em>,
San Diego, CA, USA,
pages 758-764,
6 2016.
[<a href="https://aclanthology.info/papers/S16-1117/s16-1117">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/SEMANTIC-SIMILARITY.html">Semantic Similarity</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use referential translation machines (RTMs) for predicting the semantic similarity of text in both STS Core and Cross-lingual STS. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become 14th out of 26 submissions in Cross-lingual STS. We also present rankings of various prediction tasks using the performance of RTM in terms of MRAER, a normalized relative absolute error metric.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:SEMEVAL2016]
[<a href="../Year/2016.complete.html#Bicici:RTM:SEMEVAL2016">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:WMT2016"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Referential Translation Machines for Predicting Translation Performance</strong>.
In <em>Proc. of the First Conf. on Machine Translation (WMT16)</em>,
Berlin, Germany,
pages 777-781,
8 2016.
[<a href="https://aclanthology.info/papers/W16-2382/w16-2382">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Referential translation machines (RTMs) pioneer a language independent approach for predicting translation performance and to all similarity tasks with top performance in both bilingual and monolingual settings and remove the need to access any task or domain specific information or resource. RTMs achieve to become $1$st in document-level, $4$th system at sentence-level according to mean absolute error, and $4$th in phrase-level prediction of translation quality in quality estimation task.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2016]
[<a href="../Year/2016.complete.html#Bicici:RTM:WMT2016">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2015
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:DomainFDA:PBML2015"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Domain Adaptation for Machine Translation with Instance Selection</strong>.
<em>The Prague Bulletin of Mathematical Linguistics</em>,
103:5-20,
2015.
<strong>ISSN:</strong> 1804-0462.
[doi:<a href="http://dx.doi.org/10.1515/pralin-2015-0001">10.1515/pralin-2015-0001</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/DOMAIN-ADAPTATION.html">Domain Adaptation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Domain adaptation for machine translation (MT) can be achieved by selecting training instances close to the test set from a larger set of instances. We consider $7$ different domain adaptation strategies and answer $7$ research questions, which give us a recipe for domain adaptation in MT. We perform English to German statistical MT (SMT) experiments in a setting where test and training sentences can come from different corpora and one of our goals is to learn the parameters of the sampling process. Domain adaptation with training instance selection can obtain $22\%$ increase in target $2$-gram recall and can gain up to $3.55$ BLEU points compared with random selection. Domain adaptation with feature decay algorithm (FDA) not only achieves the highest target $2$-gram recall and BLEU performance but also perfectly learns the test sample distribution parameter with correlation $0.99$. Moses SMT systems built with FDA selected 10K training sentences is able to obtain $F_1$ results as good as the baselines that use up to 2M sentences. Moses SMT systems built with FDA selected 50K training sentences is able to obtain 1 $F_1$ point better results than the baselines.</td>

</tr></table></center>
[bibtex-key = Bicici:DomainFDA:PBML2015]
[<a href="../Year/2015.complete.html#Bicici:DomainFDA:PBML2015">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:Quest:PBML2015"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Lucia Specia.
<strong>QuEst for High Quality Machine Translation</strong>.
<em>The Prague Bulletin of Mathematical Linguistics</em>,
103:43-64,
2015.
<strong>ISSN:</strong> 1804-0462.
[doi:<a href="http://dx.doi.org/10.1515/pralin-2015-0003">10.1515/pralin-2015-0003</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
In this paper we describe the use of 	extsc{QuEst}, a framework that aims to obtain predictions on the quality of translations, to improve the performance of machine translation (MT) systems without changing their internal functioning. We apply 	extsc{QuEst} to experiments with: \begin{enumerate}[label=\roman*.] 

\item multiple system translation ranking, where translations produced by different MT systems are ranked according to their estimated quality, leading to gains of up to $2.72$ BLEU, $3.66$ BLEUs, and $2.17$ $F_1$ points; 

\item n-best list re-ranking, where n-best list translations produced by an MT system are re-ranked based on predicted quality scores to get the best translation ranked top, which lead to improvements on sentence NIST score by $0.41$ points; 

\item n-best list combination, where segments from an n-best list are combined using a lattice-based re-scoring approach that minimize word error, obtaining gains of $0.28$ BLEU points; and 

\item the ITERPE strategy, which attempts to identify translation errors regardless of prediction errors (ITERPE) and build sentence-specific SMT systems (SSSS) on the ITERPE sorted instances identified as having more potential for improvement, achieving gains of up to $1.43$ BLEU, $0.54$ $F_1$, $2.9$ NIST, $0.64$ sentence BLEU, and $4.7$ sentence NIST points in English to German over the top $100$ ITERPE sorted instances. \end{enumerate}</td>

</tr></table></center>
[bibtex-key = Bicici:Quest:PBML2015]
[<a href="../Year/2015.complete.html#Bicici:Quest:PBML2015">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM_SEMEVAL"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Andy Way.
<strong>Referential translation machines for predicting semantic similarity</strong>.
<em>Language Resources and Evaluation</em>,
pp 1-27,
2015.
<strong>ISSN:</strong> 1574-020X.
[doi:<a href="http://dx.doi.org/10.1007/s10579-015-9322-7">10.1007/s10579-015-9322-7</a>]
Keyword(s): <a href="../Keyword/REFERENTIAL-TRANSLATION-MACHINE.html">Referential translation machine</a>,
<a href="../Keyword/RTM.html">RTM</a>,
<a href="../Keyword/SEMANTIC-SIMILARITY.html">Semantic similarity</a>,
<a href="../Keyword/MACHINE-TRANSLATION.html">Machine translation</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance prediction</a>,
<a href="../Keyword/MACHINE-TRANSLATION-PERFORMANCE-PREDICTION.html">Machine translation performance prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Referential translation machines (RTMs) are a computational model effective at judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants. RTMs pioneer a language-independent approach to all similarity tasks and remove the need to access any task or domain-specific information or resource. We use RTMs for predicting the semantic similarity of text and present state-of-the-art results showing that RTMs can achieve better results on the test set than on the training set. RTMs judge the quality or the semantic similarity of texts by using relevant retrieved training data as interpretants for reaching shared semantics. Interpretants are used to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs achieve top performance at SemEval in various semantic similarity prediction tasks as well as similarity prediction tasks in bilingual settings. We define MAER, mean absolute error relative to the magnitude of the target, and MRAER, mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known. RTM test performance on various tasks sorted according to MRAER can help identify which tasks and subtasks require more work by design.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM_SEMEVAL]
[<a href="../Year/2015.complete.html#Bicici:RTM_SEMEVAL">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciYuret:FDA5:TASLP"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>Optimizing Instance Selection for Statistical Machine Translation with Feature Decay Algorithms</strong>.
<em>IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP)</em>,
23:339-350,
2015.
[doi:<a href="http://dx.doi.org/10.1109/TASLP.2014.2381882">10.1109/TASLP.2014.2381882</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We introduce FDA5 for efficient parameterization, optimization, and implementation of feature decay algorithms (FDA), a class of instance selection algorithms that use feature decay. FDA increase the diversity of the selected training set by devaluing features (i.e. n-grams) that have already been included. FDA5 decides which instances to select based on three functions used for initializing and decaying feature values and scaling sentence scores controlled with $5$ parameters. We present optimization techniques that allow FDA5 to adapt these functions to in-domain and out-of-domain translation tasks for different language pairs. In a transductive learning setting, selection of training instances relevant to the test set can improve the final translation quality. In machine translation experiments performed on the $2$ million sentence English-German section of the Europarl corpus, we show that a subset of the training set selected by FDA5 can gain up to $3.22$ BLEU points compared to a randomly selected subset of the same size, can gain up to $0.41$ BLEU points compared to using all of the available training data using only $15\%$ of it, and can reach within $0.5$ BLEU points to the full training set result by using only $2.7\%$ of the full training data. FDA5 peaks at around 8M words or $15\%$ of the full training set. In an active learning setting, FDA5 minimizes the human effort by identifying the most informative sentences for translation and FDA gains up to $0.45$ BLEU points using $3/5$ of the available training data compared to using all of it and $1.12$ BLEU points compared to random training set. In translation tasks involving English and Turkish, a morphologically rich language, FDA5 can gain up to $11.52$ BLEU points compared to a randomly selected subset of the same size, can achieve the same BLEU score using as little as $4\%$ of the data compared to random instance selection, and can exceed the full dataset result by $0.78$ BLEU points. FDA5 is able to reduce the time to build a statistical machine translation system to about half with 1M words using only $3\%$ of the space for the phrase table and $8\%$ of the overall space when compared with a baseline system using all of the training data available yet still obtain only $0.58$ BLEU points difference with the baseline system in out-of-domain translation.</td>

</tr></table></center>
[bibtex-key = BiciciYuret:FDA5:TASLP]
[<a href="../Year/2015.complete.html#BiciciYuret:FDA5:TASLP">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:SEMEVAL2015"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>RTM-DCU: Predicting Semantic Similarity with Referential Translation Machines</strong>.
In <em>SemEval-2015: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation</em>,
Denver, CO, USA,
pages 56-63,
6 2015.
[<a href="https://aclanthology.info/papers/S15-2010/s15-2010">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/SEMANTIC-SIMILARITY.html">Semantic Similarity</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:SEMEVAL2015]
[<a href="../Year/2015.complete.html#Bicici:RTM:SEMEVAL2015">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:ParFDA:WMT2015"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>,
Qun Liu,
 and Andy Way.
<strong>ParFDA for Fast Deployment of Accurate Statistical Machine Translation Systems, Benchmarks, and Statistics</strong>.
In <em>Tenth Workshop on Statistical Machine Translation</em>,
Lisbon, Portugal,
pages 74-78,
9 2015.
[<a href="https://aclanthology.info/papers/W15-3005/w15-3005">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/LANGUAGE-MODELING.html">Language Modeling</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation~\cite{WMT2015} (WMT15) translation task and obtain results close to the top with an average of $3.176$ BLEU points difference using significantly less resources for building SMT systems. ParFDA is a parallel implementation of feature decay algorithms (FDA) developed for fast deployment of accurate SMT systems. ParFDA Moses SMT system we built is able to obtain the top TER performance in French to English translation. We make the data for building ParFDA Moses SMT systems for WMT15 available: \url{github.com/bicici/ParFDAWMT15}.</td>

</tr></table></center>
[bibtex-key = Bicici:ParFDA:WMT2015]
[<a href="../Year/2015.complete.html#Bicici:ParFDA:WMT2015">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:WMT2015"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>,
Qun Liu,
 and Andy Way.
<strong>Referential Translation Machines for Predicting Translation Quality and Related Statistics</strong>.
In <em>Tenth Workshop on Statistical Machine Translation</em>,
Lisbon, Portugal,
pages 304-308,
9 2015.
[<a href="https://aclanthology.info/papers/W15-3035/w15-3035">WWW</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use referential translation machines (RTMs) for predicting translation performance. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. We improve our RTM models with the ParFDA instance selection model~\cite{Bicici:FDA54FDA:WMT15}, with additional features for predicting the translation performance, and with improved learning models. We develop RTM models for each WMT15 QET (QET15) subtask and obtain improvements over QET14 results. RTMs achieve top performance in QET15 ranking $1$st in document- and sentence-level prediction tasks and $2$nd in word-level prediction task.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2015]
[<a href="../Year/2015.complete.html#Bicici:RTM:WMT2015">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2014
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:ParFDA:WMT2014"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>,
Qun Liu,
 and Andy Way.
<strong>Parallel FDA5 for Fast Deployment of Accurate Statistical Machine Translation Systems</strong>.
In <em>Ninth Workshop on Statistical Machine Translation</em>,
Baltimore, MD, USA,
pages 59-65,
6 2014.
[<a href="https://aclanthology.info/papers/W14-3303/w14-3303">WWW</a>]
[<a href="http://bicici.github.io/publications/2014/ParFDA5forFDASMT.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/LANGUAGE-MODELING.html">Language Modeling</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation direction. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of $3.49$ BLEU points difference using significantly less resources for training and development.</td>

</tr></table></center>
[bibtex-key = Bicici:ParFDA:WMT2014]
[<a href="../Year/2014.complete.html#Bicici:ParFDA:WMT2014">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:SEMEVAL2014"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Andy Way.
<strong>RTM-DCU: Referential Translation Machines for Semantic Similarity</strong>.
In <em>SemEval-2014: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation</em>,
Dublin, Ireland,
pages 487-496,
8 2014.
[<a href="https://aclanthology.info/papers/S14-2085/s14-2085">WWW</a>]
[<a href="http://bicici.github.io/publications/2014/RTM_STS.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/SEMANTIC-SIMILARITY.html">Semantic Similarity</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs judge the quality or the semantic similarity of text by using retrieved relevant training data as interpretants for reaching shared semantics. We derive features measuring the closeness of the test sentences to the training data via interpretants, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs provide a language independent solution to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in the semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:SEMEVAL2014]
[<a href="../Year/2014.complete.html#Bicici:RTM:SEMEVAL2014">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:WMT2014"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Andy Way.
<strong>Referential Translation Machines for Predicting Translation Quality</strong>.
In <em>Ninth Workshop on Statistical Machine Translation</em>,
Baltimore, MD, USA,
pages 313-321,
6 2014.
[<a href="https://aclanthology.info/papers/W14-3339/w14-3339">WWW</a>]
[<a href="http://bicici.github.io/publications/2014/RTMforQE.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations and achieve the top performance in WMT13 quality estimation task (QET13). We improve our RTM models with the Parallel FDA5 instance selection model, with additional features for predicting the translation performance, and with improved learning models. We develop RTM models for each WMT14 QET (QET14) subtask, obtain improvements over QET13 results, and rank $1$st in all of the tasks and subtasks of QET14.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2014]
[<a href="../Year/2014.complete.html#Bicici:RTM:WMT2014">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="QTLPD2.2.2"></a>Ergun Biçici et al..
<strong>Quality Estimation for Extending Good Translations</strong>.
Technical report,
Dublin City Univ.,
2014.
Note: EU project QTLaunchPad D2.2.2: www.qt21.eu/launchpad/content/delivered.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We present experiments using quality estimation models to improve the performance of statistical machine translation (SMT) systems by supplementing their training corpora or by building sentence-specific SMT models for instances predicted as having potential for improvement by the ITERPE model. The experiments with quality-informed active learning strategy select, among alternative machine translations, those which are: (i) predicted to have high quality, and thus can be added to the machine translation system training set; (ii) predicted to have low quality, and thus need to be corrected/translated by humans, with the human corrections added to the machine translation system training set. Improvement is measured by the increase in the performance of the overall machine translation systems on held-out datasets, where performance is measured by automatic evaluation metrics comparing the scores of the original machine translation system against the score of the improved machine translation system after additional material is used. The experiments with ITERPE consist in automatically grouping translation instances into different quality bands, for instance for re-translation or for post-editing [Bicici and Specia, 2014]. This method can be helpful in automatic identification of quality barriers in MT to achieve high quality machine translation.</td>

</tr></table></center>
[bibtex-key = QTLPD2.2.2]
[<a href="../Year/2014.complete.html#QTLPD2.2.2">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="QTLPD2.2.1"></a>Ergun Biçici et al..
<strong>Quality Estimation for System Selection and Combination</strong>.
Technical report,
Dublin City Univ.,
2014.
Note: EU project QTLaunchPad D2.2.1: www.qt21.eu/launchpad/content/delivered.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We present experiments using state of the art quality estimation models to improve the performance of machine translation systems without changing the internal functioning of such systems. The experiments include the following approaches: (i) n-best list re-ranking, where translation candidates (segments) produced by a machine translation system are re-ranked based on predicted quality scores such as to get the best translation ranked top; (ii) n-best list recombination, where sub-segments from the n-best list are mixed using a lattice-based approach, and the complete generated segments are scored using quality predictions and then re-ranked as in (i); (iii) system selection, where translations produced by multiple machine translation systems and a human translator are sorted according to predicted quality to select the best translated segment, including the challenging case where the source of the translation (i.e., which system/human produced it) is unknown, and (iv) diagnosis of statistical machine translation systems by looking at internal features of the decoder and their correlation with translation quality, as well as using them to predict groups of errors in the translations.</td>

</tr></table></center>
[bibtex-key = QTLPD2.2.1]
[<a href="../Year/2014.complete.html#QTLPD2.2.1">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciIDF_ITERPE2014"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>ITERPE: Identifying Translation Errors Regardless of Prediction Errors</strong>.
 Invention Disclosure,
2014.
Note: Invention Disclosure, DCU Invent Innovation and Enterprise: www.dcu.ie/invent/.
[bibtex-key = BiciciIDF_ITERPE2014]
[<a href="../Year/2014.complete.html#BiciciIDF_ITERPE2014">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2013
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:MTPP:MTJ2013"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>,
Declan Groves,
 and Josef van Genabith.
<strong>Predicting Sentence Translation Quality Using Extrinsic and Language Independent Features</strong>.
<em>Machine Translation</em>,
27(3-4):171-192,
2013.
<strong>ISSN:</strong> 0922-6567.
[doi:<a href="http://dx.doi.org/10.1007/s10590-013-9138-4">10.1007/s10590-013-9138-4</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We develop a top performing model for automatic, accurate, and language independent prediction of sentence-level statistical machine translation (SMT) quality with or without looking at the translation outputs. We derive various feature functions measuring the closeness of a given test sentence to the training data and the difficulty of translating the sentence. We describe 	exttt{mono} feature functions that are based on statistics of only one side of the parallel training corpora and 	exttt{duo} feature functions that incorporate statistics involving both source and target sides of the training data. Overall, we describe novel, language independent, and SMT system extrinsic features for predicting the SMT performance, which also rank high during feature ranking evaluations. 

We experiment with different learning settings, with or without looking at the translations, which help differentiate the contribution of different feature sets. We apply partial least squares and feature subset selection, both of which improve the results and we present ranking of the top features selected for each learning setting, providing an exhaustive analysis of the extrinsic features used. We show that by just looking at the test source sentences and not using the translation outputs at all, we can achieve better performance than a baseline system using SMT model dependent features that generated the translations. Furthermore, our prediction system is able to achieve the $2$nd best performance overall according to the official results of the Quality Estimation Task (QET) challenge when also looking at the translation outputs. Our representation and features achieve the top performance in QET among the models using the SVR learning model.</td>

</tr></table></center>
[bibtex-key = Bicici:MTPP:MTJ2013]
[<a href="../Year/2013.complete.html#Bicici:MTPP:MTJ2013">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:ParFDA:WMT2013"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Feature Decay Algorithms for Fast Deployment of Accurate Statistical Machine Translation Systems</strong>.
In <em>Eighth Workshop on Statistical Machine Translation</em>,
Sofia, Bulgaria,
pages 78-84,
8 2013.
[<a href="http://bicici.github.io/publications/2013/FDAforFDA.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/LANGUAGE-MODELING.html">Language Modeling</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use feature decay algorithms (FDA) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction. We develop parallel FDA for solving computational scalability problems caused by the abundance of training data for SMT models and language models and still achieve SMT performance that is on par with using all of the training data or better. Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later. Parallel FDA can also be used for selecting the LM corpus based on the training set selected by parallel FDA. The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems. The relevancy of the selected LM corpus can reach up to $86\%$ reduction in the number of OOV tokens and up to $74\%$ reduction in the perplexity. We perform SMT experiments in all language pairs in the WMT13 translation task and obtain SMT performance close to the top systems using significantly less resources for training and development.</td>

</tr></table></center>
[bibtex-key = Bicici:ParFDA:WMT2013]
[<a href="../Year/2013.complete.html#Bicici:ParFDA:WMT2013">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM:WMT2013"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Referential Translation Machines for Quality Estimation</strong>.
In <em>Eighth Workshop on Statistical Machine Translation</em>,
Sofia, Bulgaria,
pages 343-351,
8 2013.
[<a href="https://aclanthology.info/papers/W13-2242/w13-2242">WWW</a>]
[<a href="http://bicici.github.io/publications/2013/RTMforQE.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We introduce referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for estimating the quality of translation outputs, judging the semantic similarity between text, and evaluating the quality of student answers. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations. We develop novel techniques for solving all subtasks in the WMT13 quality estimation (QE) task (QET2013) based on individual RTM models. Our results achieve improvements over last year's QE task results (QET 2012), as well as our previous results, provide new features and techniques for QE, and rank $1$st or $2$nd in all of the subtasks.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM:WMT2013]
[<a href="../Year/2013.complete.html#Bicici:RTM:WMT2013">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM_STS:SEMEVAL2013"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Josef van Genabith.
<strong>CNGL-CORE: Referential Translation Machines for Measuring Semantic Similarity</strong>.
In <em>*SEM 2013: The Second Joint Conf. on Lexical and Computational Semantics</em>,
Atlanta, GA, USA,
pages 234-240,
6 2013.
[<a href="https://aclanthology.info/papers/S13-1034/s13-1034">WWW</a>]
[<a href="http://bicici.github.io/publications/2013/STS_RTM.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>,
<a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for judging the semantic similarity between text. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view semantic similarity as paraphrasing between any two given texts. Each view is modeled by an RTM model, giving us a new perspective on the binary relationship between the two. Our prediction model is the 15th on some tasks and 30th overall out of 89 submissions in total according to the official results of the Semantic Textual Similarity (STS 2013) challenge.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM_STS:SEMEVAL2013]
[<a href="../Year/2013.complete.html#Bicici:RTM_STS:SEMEVAL2013">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:RTM_SRA:SEMEVAL2013"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Josef van Genabith.
<strong>CNGL: Grading Student Answers by Acts of Translation</strong>.
In <em>SemEval-2013: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation</em>,
Atlanta, GA, USA,
pages 585-591,
6 2013.
[<a href="https://aclanthology.info/papers/S13-2098/s13-2098">WWW</a>]
[<a href="http://bicici.github.io/publications/2013/SRA_AOT.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view question answering as translation from the question to the answer, from the question to the reference answer, from the answer to the reference answer, or from the question and the answer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the 2nd best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge.</td>

</tr></table></center>
[bibtex-key = Bicici:RTM_SRA:SEMEVAL2013]
[<a href="../Year/2013.complete.html#Bicici:RTM_SRA:SEMEVAL2013">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="QTLPD3.2.1"></a>Ergun Biçici et al..
<strong>Definition of Interfaces</strong>.
Technical report,
Dublin City Univ.,
2013.
Note: EU project QTLaunchPad D3.2.1: www.qt21.eu/launchpad/content/delivered.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
The aim of this report is to define the interfaces for the tools used in the MT development and evaluation scenarios as included in the QTLaunchPad (QTLP) infrastructure. Specification of the interfaces is important for the interaction and interoperability of the tools in the developed QTLP infrastructure. In addressing this aim, the report provides: 1. Descriptions of the common aspects of the tools and their standardized data formatsÍ¾ 2. Descriptions of the interfaces for the tools for interoperability. where the tools are categorized into preparation, development, and evaluation categories including the human interfaces for quality assessment with multidimensional quality metrics. Interface specifications allow a modular tool infrastructure, flexibly selecting among alternative implementations, enabling realistic expectations to be made at different sections of the QTLP information flow pipeline, and supporting the QTLP infrastructure. D3.2.1 allows the emergence of the QTLP infrastructure and helps the identification and acquisition of existing tools (D4.4.1), the integration of identified language processing tools (D3.3.1), their implementation (D3.4.1), and their testing (D3.5.1). QTLP infrastructure will facilitate the organization and running of the quality translation shared task (D5.2.1). We also provide human interfaces for translation quality assessment with the multidimensional quality metrics (D1.1.1). D3.2.1 is a living document until M12, which is when the identification and acquisition of existing tools (D4.4.1) and the implementation of identified language processing tools (D3.4.1) are due.</td>

</tr></table></center>
[bibtex-key = QTLPD3.2.1]
[<a href="../Year/2013.complete.html#QTLPD3.2.1">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="QTLPD3.1.1"></a>Ergun Biçici et al..
<strong>Definition of Machine Translation and Evaluation Workflows</strong>.
Technical report,
Dublin City Univ.,
2013.
Note: EU project QTLaunchPad D3.1.1: www.qt21.eu/launchpad/content/delivered.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>.
[bibtex-key = QTLPD3.1.1]
[<a href="../Year/2013.complete.html#QTLPD3.1.1">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="QTLPD2.1.3"></a>Ergun Biçici et al..
<strong>Quality Estimation for Dissemination</strong>.
Technical report,
Dublin City Univ.,
2013.
Note: EU project QTLaunchPad D2.1.3: www.qt21.eu/launchpad/content/delivered.
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/PERFORMANCE-PREDICTION.html">Performance Prediction</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We present benchmarking experiments for the intrinsic and extrinsic evaluation of an extended version of our open source framework for machine translation quality estimation QUEST, which is described in D2.1.2. We focus on the application of quality predictions for dissemination by estimating post-editing effort. As an extrinsic task, we use quality predictions to rank alternative translations from multiple MT systems according to their estimated quality. Additionally, we experiment with a small dataset annotated for quality labels with different levels of granularity in a attempt to predict multidimensional quality metric (MQM) scores.</td>

</tr></table></center>
[bibtex-key = QTLPD2.1.3]
[<a href="../Year/2013.complete.html#QTLPD2.1.3">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciIDF_RTM2013"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Referential Translation Machines</strong>.
 Invention Disclosure,
2013.
Note: Invention Disclosure, DCU Invent Innovation and Enterprise: www.dcu.ie/invent/.
[bibtex-key = BiciciIDF_RTM2013]
[<a href="../Year/2013.complete.html#BiciciIDF_RTM2013">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2012
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="BiciciIDF_MTPP2012"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Machine Translation Performance Predictor</strong>.
 Invention Disclosure,
2012.
Note: Invention Disclosure, DCU Invent Innovation and Enterprise: www.dcu.ie/invent/.
[bibtex-key = BiciciIDF_MTPP2012]
[<a href="../Year/2012.complete.html#BiciciIDF_MTPP2012">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2011
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="RegMTBook"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>The Regression Model of Machine Translation: Learning, Instance Selection, Decoding, and Evaluation</strong>.
LAP LAMBERT Academic Publishing,
2011.
<strong>ISBN:</strong> 3846507490.
[<a href="http://www.amazon.com/Regression-Model-Machine-Translation-Evaluation/dp/3846507490">WWW</a>]
[bibtex-key = RegMTBook]
[<a href="../Year/2011.complete.html#RegMTBook">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciThesis"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>The Regression Model of Machine Translation</strong>.
PhD thesis,
KoÃ§ University,
2011.
Note: Supervisor: Deniz Yuret.
[<a href="http://bicici.github.io/publications/2011/RegMTThesis_Web.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>,
<a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Machine translation is the task of automatically finding the translation of a source sentence in the target language. Statistical machine translation (SMT) use parallel corpora or bilingual paired corpora that are known to be translations of each other to find a likely translation for a given source sentence based on the observed translations. The task of machine translation can be seen as an instance of estimating the functions that map strings to strings. 

Regression based machine translation (RegMT) approach provides a learning framework for machine translation, separating learning models for training, training instance selection, feature representation, and decoding. We use the transductive learning framework for making the RegMT approach computationally more scalable and consider the model building step independently for each test sentence. We develop training instance selection algorithms that not only make RegMT computationally more scalable but also improve the performance of standard SMT systems. We develop better training instance selection techniques than previous work from given parallel training sentences for achieving more accurate RegMT models using less training instances. 

We introduce L1 regularized regression as a better model than L2 regularized regression for statistical machine translation. Our results demonstrate that sparse regression models are better than L2 regularized regression for statistical machine translation in predicting target features, estimating word alignments, creating phrase tables, and generating translation outputs. We develop good evaluation techniques for measuring the performance of the RegMT model and the quality of the translations. We use F1 measure, which performs good when evaluating translations into English according to human judgments. F1 allows us to evaluate the performance of the RegMT models using the target feature prediction vectors or the coefficients matrices learned or a given SMT model using its phrase table without performing the decoding step, which can be computationally expensive. 

Decoding is dependent on the representation of the training set and the features used. We use graph decoding on the prediction vectors represented in $n$-gram or word sequence counts space found in the training set. We also decode using Moses after transforming the learned weight matrix representing the mappings between the source and target features to a phrase table that can be used by Moses during decoding. We demonstrate that sparse L1 regularized regression performs better than L2 regularized regression in the German-English translation task and in the Spanish-English translation task when using small sized training sets. Graph based decoding can provide an alternative to phrase-based decoding in translation domains having low vocabulary.</td>

</tr></table></center>
[bibtex-key = BiciciThesis]
[<a href="../Year/2011.complete.html#BiciciThesis">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciYuret:ISforMT:WMT11"></a>Ergun BiÃ§ici and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>Instance Selection for Machine Translation using Feature Decay Algorithms</strong>.
In <em>Sixth Workshop on Statistical Machine Translation</em>,
Edinburgh, Scotland,
pages 272-283,
7 2011.
[<a href="http://www.aclweb.org/anthology/W11-2131">WWW</a>]
[<a href="bicici.github.io/publications/2011/ISforMTFDA.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We present an empirical study of instance selection techniques for machine translation. In an active learning setting, instance selection minimizes the human effort by identifying the most informative sentences for translation. In a transductive learning setting, selection of training instances relevant to the test set improves the final translation quality. After reviewing the state of the art in the field, we generalize the main ideas in a class of instance selection algorithms that use feature decay. Feature decay algorithms increase diversity of the training set by devaluing features that are already included. We show that the feature decay rate has a very strong effect on the final translation quality whereas the initial feature values, inclusion of higher order features, or sentence length normalizations do not. We evaluate the best instance selection methods using a standard Moses baseline using the whole 1.6 million sentence English-German section of the Europarl corpus. We show that selecting the best 3000 training sentences for a specific test sentence is sufficient to obtain a score within 1 BLEU of the baseline, using 5\% of the training data is sufficient to exceed the baseline, and a ~2 BLEU improvement over the baseline is possible by optimally selected subset of the training data. In out-of-domain translation, we are able to reduce the training set size to about 7\% and achieve a similar performance with the baseline.</td>

</tr></table></center>
[bibtex-key = BiciciYuret:ISforMT:WMT11]
[<a href="../Year/2011.complete.html#BiciciYuret:ISforMT:WMT11">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciYuret:RegMT:WMT11"></a>Ergun BiÃ§ici and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>RegMT System for Machine Translation, System Combination, and Evaluation</strong>.
In <em>Sixth Workshop on Statistical Machine Translation</em>,
Edinburgh, Scotland,
pages 323-329,
7 2011.
[<a href="http://www.aclweb.org/anthology/W11-2137">WWW</a>]
[<a href="bicici.github.io/publications/2011/RegMTSystem.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We present the results we obtain using our RegMT system, which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs. We present results with our instance selection methods that perform feature decay for proper selection of training instances, which plays an important role to learn correct feature mappings. RegMT uses L2 regularized regression as well as L1 regularized regression for sparse regression estimation of target features.We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with F1 measure over target features.</td>

</tr></table></center>
[bibtex-key = BiciciYuret:RegMT:WMT11]
[<a href="../Year/2011.complete.html#BiciciYuret:RegMT:WMT11">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2010
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="BiciciKozat2010:WMT"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and S. Serdar Kozat.
<strong>Adaptive Model Weighting and Transductive Regression for Predicting Best System Combinations</strong>.
In <em>ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR</em>,
Uppsala, Sweden,
7 2010.
[<a href="http://www.aclweb.org/anthology/W/W10/W10-1740.pdf">PDF</a>]
[<a href="bicici.github.io/publications/2010/WMT2010_TRBMT_CSMT.pdf">PDF</a>]
[<a href="bicici.github.io/publications/2010/WMT2010_TRBMT_CSMT.ps">POSTSCRIPT</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We analyze adaptive model weighting techniques for reranking using instance scores obtained by L1 regularized transductive regression. Competitive statistical machine translation is an on-line learning technique for sequential translation tasks where we try to select the best among competing statistical machine translators. The competitive predictor assigns a probability per model weighted by the sequential performance. We define additive, multiplicative, and loss-based weight updates with exponential loss functions for competitive statistical machine translation. Without any pre-knowledge of the performance of the translation models, we succeed in achieving the performance of the best model in all systems and surpass their performance in most of the language pairs we considered.</td>

</tr></table></center>
[bibtex-key = BiciciKozat2010:WMT]
[<a href="../Year/2010.complete.html#BiciciKozat2010:WMT">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciYuret2010:CSW"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>L1 Regularization for Learning Word Alignments in Sparse Feature Matrices</strong>.
In <em>Computer Science Student Workshop, CSW'10</em>,
Koc University, Istinye Campus, Istanbul, Turkey,
2 2010.
[<a href="http://myweb.sabanciuniv.edu/csw/">WWW</a>]
[<a href="http://bicici.github.io/publications/2010/L1forWordAlignment.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Sparse feature representations can be used in various domains. We compare the effectiveness of L1 regularization techniques for regression to learn mappings between features given in a sparse feature matrix. We apply these techniques for learning word alignments commonly used for machine translation. The performance of the learned mappings are measured using the phrase table generated on a larger corpus by a state of the art word aligner. The results show the effectiveness of using L1 regularization versus L2 used in ridge regression.</td>

</tr></table></center>
[bibtex-key = BiciciYuret2010:CSW]
[<a href="../Year/2010.complete.html#BiciciYuret2010:CSW">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciYuret2010:WMT"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>L1 Regularized Regression for Reranking and System Combination in Machine Translation</strong>.
In <em>ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR</em>,
Uppsala, Sweden,
7 2010.
[<a href="http://www.aclweb.org/anthology/W/W10/W10-1741.pdf">PDF</a>]
[<a href="bicici.github.io/publications/2010/WMT2010_L1forSMTReranking.pdf">PDF</a>]
[<a href="bicici.github.io/publications/2010/WMT2010_L1forSMTReranking.ps">POSTSCRIPT</a>]
Keyword(s): <a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We use L1 regularized transductive regression to learn mappings between source and target features of the training sets derived for each test sentence and use these mappings to rerank translation outputs. We compare the effectiveness of L1 regularization techniques for regression to learn mappings between features given in a sparse feature matrix. The results show the effectiveness of using L1 regularization versus L2 used in ridge regression. We show that regression mapping is effective in reranking translation outputs and in selecting the best system combinations with encouraging results on different language pairs.</td>

</tr></table></center>
[bibtex-key = BiciciYuret2010:WMT]
[<a href="../Year/2010.complete.html#BiciciYuret2010:WMT">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2009
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="yuret-bicici:2009:Short"></a><a href="../Author/YURET-D.html">Deniz Yuret</a> and <a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Modeling Morphologically Rich Languages Using Split Words and Unstructured Dependencies</strong>.
In <em>ACL-IJCNLP 2009 Conf. Short Papers</em>,
Suntec, Singapore,
pages 345-348,
8 2009.
[<a href="http://www.aclweb.org/anthology/P/P09/P09-2087">WWW</a>]
[<a href="bicici.github.io/publications/2009/MMRLACL.pdf">PDF</a>]
[<a href="bicici.github.io/publications/2009/MMRLACL.ps">POSTSCRIPT</a>]
Keyword(s): <a href="../Keyword/LANGUAGE-MODELING.html">Language Modeling</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We experiment with splitting words into their stem and suffix components for modeling morphologically rich languages. We show that using a morphological analyzer and disambiguator results in a significant perplexity reduction in Turkish. We present flexible n-gram models, FlexGrams, which assume that the n-1 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n-1 positions. Our final model achieves 27\% perplexity reduction compared to the standard n-gram model.</td>

</tr></table></center>
[bibtex-key = yuret-bicici:2009:Short]
[<a href="../Year/2009.complete.html#yuret-bicici:2009:Short">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2008
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:JMAGS07"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Consensus Ontologies in Socially Interacting MultiAgent Systems</strong>.
<em>Multiagent and Grid Systems - An International Journal of Cloud Computing</em>,
4(3):297-314,
2008.
[<a href="content.iospress.com/articles/multiagent-and-grid-systems/mgs00106">WWW</a>]
[<a href="http://bicici.github.io/publications/2007/JMAGS.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/CONSENSUS-ONTOLOGY.html">Consensus Ontology</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
This paper presents approaches for building, managing, and evaluating consensus ontologies from the individual ontologies of a network of socially interacting agents. Each agent has its own conceptualization of the world within the multiagent system framework. The interactions between agents are modeled by sending queries and receiving responses and later assessing each other's performance based on the results. This model enables us to measure the \emph{quality} of the societal beliefs in the resources which we represent as the \emph{expertise} in each domain. The dynamic nature of our system allows us to model the emergence of consensus that mimics the evolution of language. We present an algorithm for generating the consensus ontologies which makes use of the authoritative agent's conceptualization in a given domain. As the expertise of agents changes after a number of interactions, the consensus ontology that we build based on the agents' individual views evolves. The resulting approach is concordant with the principles of emergent semantics. We provide formal definitions for the problem of finding a consensus ontology in a step by step manner. We evaluate the consensus ontologies by using different heuristic measures of similarity based on the component ontologies. Conceptual processing methods for generating, manipulating, and evaluating consensus ontologies are given and experimental results are presented. The presented approach looks promising and opens new directions for further research.</td>

</tr></table></center>
[bibtex-key = Bicici:JMAGS07]
[<a href="../Year/2008.complete.html#Bicici:JMAGS07">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:CICLing08"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Context-Based Sentence Alignment in Parallel Corpora</strong>.
<em>Lecture Notes in Computer Science</em>,
4919:434-444,
2008.
Note: 9th International Conf. on Intelligent Text Processing and Computational Linguistics (CICLing 2008).
<strong>ISBN:</strong> 978-3-540-78135-6.
[<a href="http://dx.doi.org/10.1007/978-3-540-78135-6_37">WWW</a>]
[<a href="bicici.github.io/publications/2008/ContextBasedSAPC.pdf">PDF</a>]
[doi:<a href="http://dx.doi.org/10.1007/978-3-540-78135-6_37">10.1007/978-3-540-78135-6_37</a>]
Keyword(s): <a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>,
<a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
This paper presents a language-independent context-based sentence alignment technique given parallel corpora. We can view the problem of aligning sentences as finding translations of sentences chosen from different sources. Unlike current approaches which rely on pre-defined features and models, our algorithm employs features derived from the distributional properties of words and does not use any language dependent knowledge. We make use of the context of sentences and the notion of Zipfian word vectors which effectively models the distributional properties of words in a given sentence. We accept the context to be the frame in which the reasoning about sentence alignment is done. We evaluate the performance of our system based on two different measures: sentence alignment accuracy and sentence alignment coverage. We compare the performance of our system with commonly used sentence alignment systems and show that our system performs 1.2149 to 1.6022 times better in reducing the error rate in alignment accuracy and coverage for moderately sized corpora.</td>

</tr></table></center>
[bibtex-key = Bicici:CICLing08]
[<a href="../Year/2008.complete.html#Bicici:CICLing08">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciDymetman:CICLing08"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Marc Dymetman.
<strong>Dynamic Translation Memory: Using Statistical Machine Translation to improve Translation Memory Fuzzy Matches</strong>.
<em>Lecture Notes in Computer Science</em>,
4919:454-465,
2008.
Note: 9th International Conf. on Intelligent Text Processing and Computational Linguistics (CICLing 2008).
<strong>ISBN:</strong> 978-3-540-78135-6.
[<a href="http://dx.doi.org/10.1007/978-3-540-78135-6_39">WWW</a>]
[<a href="http://bicici.github.io/publications/2008/DTM.pdf">PDF</a>]
[doi:<a href="http://dx.doi.org/10.1007/978-3-540-78135-6_39">10.1007/978-3-540-78135-6_39</a>]
Keyword(s): <a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>,
<a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>,
<a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Professional translators of technical documents often use Translation Memory (TM) systems in order to capitalize on the repetitions frequently observed in these documents. TM systems typically exploit not only complete matches between the source sentence to be translated and some previously translated sentence, but also so-called \emph{fuzzy matches}, where the source sentence has some substantial commonality with a previously translated sentence. These fuzzy matches can be very worthwhile as a starting point for the human translator, but the translator then needs to manually edit the associated TM-based translation to accommodate the differences with the source sentence to be translated. If part of this process could be automated, the cost of human translation could be significantly reduced. The paper proposes to perform this automation in the following way: a phrase-based Statistical Machine Translation (SMT) system (trained on a bilingual corpus in the same domain as the TM) is combined with the TM fuzzy match, by extracting from the fuzzy-match a large (possibly gapped) bi-phrase that is dynamically added to the usual set of ''static'' bi-phrases used for decoding the source. We report experiments that show significant improvements in terms of BLEU and NIST scores over both the translations produced by the stand-alone SMT system and the fuzzy-match translations proposed by the stand-alone TM system.</td>

</tr></table></center>
[bibtex-key = BiciciDymetman:CICLing08]
[<a href="../Year/2008.complete.html#BiciciDymetman:CICLing08">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciDymetmanDTM2008"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Marc Dymetman.
<strong>Dynamic translation memory using statistical machine translation</strong>.
 US Patent 8,244,519,
2008.
[bibtex-key = BiciciDymetmanDTM2008]
[<a href="../Year/2008.complete.html#BiciciDymetmanDTM2008">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2007
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:CONTEXT07"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Local Context Selection for Aligning Sentences in Parallel Corpora</strong>.
<em>Lecture Notes in Artificial Intelligence</em>,
4635:82-93,
2007.
Note: 6th International and Interdisciplinary Conf. on Modeling and Using Context (CONTEXT 2007).
[<a href="http://bicici.github.io/publications/2007/ContextSelectionAModels.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>,
<a href="../Keyword/MACHINE-TRANSLATION.html">Machine Translation</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
This paper presents a novel language independent context-based sentence alignment technique given parallel corpora. We can view the problem of aligning sentences as finding translations of sentences chosen from different sources. Unlike current approaches which rely on pre-defined features and models, our algorithm employs features derived from the distributional properties of sentences and does not use any language dependent knowledge. We make use of the context of sentences and introduce the notion of Zipfian word vectors which effectively models the distributional properties of a given sentence. We accept the context to be the frame in which the reasoning about sentence alignment is done. We examine alternatives for local context models and demonstrate that our context based sentence alignment algorithm performs better than prominent sentence alignment techniques. Our system dynamically selects the local context for a pair of set of sentences which maximizes the correlation. We evaluate the performance of our system based on two different measures: sentence alignment accuracy and sentence alignment coverage. We compare the performance of our system with commonly used sentence alignment systems and show that our system performs 1.1951 to 1.5404 times better in reducing the error rate in alignment accuracy and coverage.</td>

</tr></table></center>
[bibtex-key = Bicici:CONTEXT07]
[<a href="../Year/2007.complete.html#Bicici:CONTEXT07">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:ICANNGA07"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>Locally Scaled Density Based Clustering</strong>.
<em>Lecture Notes in Computer Science</em>,
4431:739-748,
2007.
Note: <a href=../2007/LSDBC/ICANNGATalk.pdf>Presentation</a> <br> <a href=../2007/LSDBC/CLSDBC_README.txt>LSDBC README</a> <br> <a href=../2007/LSDBC/clsdbc>LSDBC Linux Executable</a> (For academic and research purposes) <br> <a href=../2007/LSDBC/clsdbcnl>LSDBC Linux Executable (no noise detection)</a> (For academic and research purposes).
<strong>ISBN:</strong> 978-3-540-71589-4.
[<a href="http://bicici.github.io/publications/2007/LSDBC/LSDBC-icannga07.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Density based clustering methods allow the identification of arbitrary, not necessarily convex regions of data points that are densely populated. The number of clusters does not need to be specified beforehand; a cluster is defined to be a connected region that exceeds a given density threshold. This paper introduces the notion of local scaling in density based clustering, which determines the density threshold based on the local statistics of the data. The local maxima of density are discovered using a k-nearest-neighbor density estimation and used as centers of potential clusters. Each cluster is grown until the density falls below a pre-specified ratio of the center point's density. The resulting clustering technique is able to identify clusters of arbitrary shape on noisy backgrounds that contain significant density gradients. The focus of this paper is to automate the process of clustering by making use of the local density information for arbitrarily sized, shaped, located, and numbered clusters. The performance of the new algorithm is promising as it is demonstrated on a number of synthetic datasets and images for a wide range of its parameters.</td>

</tr></table></center>
[bibtex-key = Bicici:ICANNGA07]
[<a href="../Year/2007.complete.html#Bicici:ICANNGA07">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2006
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:COMPSAC2006"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Consensus Ontology Generation in a Socially Interacting MultiAgent System</strong>.
In <em>30th Annual International Computer Software and Applications Conf. (COMPSAC'06)</em>,
Chicago, IL, USA,
pages 279-284,
9 2006.
IEEE Computer Society.
Note: <a href=../2006/COMPSACTalk.pdf>Presentation</a>.
[<a href="http://bicici.github.io/publications/2006/COMPSAC2006Extended.pdf">PDF</a>]
[doi:<a href="http://dx.doi.org/10.1109/COMPSAC.2006.126">10.1109/COMPSAC.2006.126</a>]
Keyword(s): <a href="../Keyword/CONSENSUS-ONTOLOGY.html">Consensus Ontology</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
This paper presents an approach for building consensus ontologies from the individual ontologies of a network of socially interacting agents. Each agent has its own conceptualization of the world. The interactions between agents are modeled by sending queries and receiving responses and later assessing each otherâ€™s performance based on the results. This model enables us to measure the quality of the societal beliefs in the resources which we represent as the expertise in each domain. The dynamic nature of our system allows us to model the emergence of consensus that mimics the evolution of language. We present an algorithm for generating the consensus ontologies which makes use of the authoritative agentâ€™s conceptualization in a given domain. As the expertise of agents change after a number of interactions, the consensus ontology that we build based on the agentsâ€™ individual views evolves. We evaluate the consensus ontologies by using different heuristic measures of similarity based on the component ontologies.</td>

</tr></table></center>
[bibtex-key = Bicici:COMPSAC2006]
[<a href="../Year/2006.complete.html#Bicici:COMPSAC2006">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="Bicici:AMTA2006"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Generating Consensus Ontologies among Socially Interacting Agents</strong>.
In <em>International Workshop on Agents and Multiagent Systems, from Theory to Application (AMTA 2006)</em>,
Québec City, Canada,
6 2006.
[<a href="http://bicici.github.io/publications/2006/AMTA2006Extended.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/CONSENSUS-ONTOLOGY.html">Consensus Ontology</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
This paper presents an approach for building consensus ontologies from the individual ontologies of a network of socially interacting agents. Each agent has its own conceptualization of the world. The interactions between agents are modeled by sending queries and receiving responses and later assessing each otherâ€™s performance based on the results. This model enables us to measure the quality of the societal beliefs in the resources which we represent as the expertise in each domain. The dynamic nature of our system allows us to model the emergence of consensus that mimics the evolution of language. We present an algorithm for generating the consensus ontologies which makes use of the authoritative agentâ€™s conceptualization in a given domain. As the expertise of agents change after a number of interactions, the consensus ontology that we build based on the agentsâ€™ individual views evolves. We evaluate the consensus ontologies by using different heuristic measures of similarity based on the component ontologies.</td>

</tr></table></center>
[bibtex-key = Bicici:AMTA2006]
[<a href="../Year/2006.complete.html#Bicici:AMTA2006">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="BiciciTAINN06"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and <a href="../Author/YURET-D.html">Deniz Yuret</a>.
<strong>Clustering Word Pairs to Answer Analogy Questions</strong>.
In <em>Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN '06)</em>,
Akyaka, Mugla,
pages 277-284,
6 2006.
Note: <a href=../2006/TAINNTalk.pdf>Presentation</a>.
[<a href="http://bicici.github.io/publications/2006/LAWSQ-LNCS.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We focus on answering word analogy questions by using clustering techniques. The increased performance in answering word similarity questions can have many possible applications, including question answering and information retrieval. We present an analysis of clustering algorithmsâ€™ performance on answering word similarity questions. This paperâ€™s contributions can be summarized as: (i) casting the problem of solving word analogy questions as an instance of learning clusterings of data and measuring the effectiveness of prominent clustering techniques in learning semantic relations; (ii) devising a heuristic approach to combine the results of different clusterings for the purpose of distinctly separating word pair semantics; (iii) answering SAT-type word similarity questions using our technique.</td>

</tr></table></center>
[bibtex-key = BiciciTAINN06]
[<a href="../Year/2006.complete.html#BiciciTAINN06">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2005
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="FormalConsensus"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Formal Consensus</strong>.
Research Note,
2005.
Note: Technical Report.
Keyword(s): <a href="../Keyword/CONSENSUS-ONTOLOGY.html">Consensus Ontology</a>.
[bibtex-key = FormalConsensus]
[<a href="../Year/2005.complete.html#FormalConsensus">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2004
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:DrugAER"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Relational Learning from Drug Adverse Events Reports</strong>.
In <em>North Carolina Symposium on Biotechnology & Bioinformatics, IEEE Tech4Life</em>,
2004.
Note: <a href=../2004/IEEETech4LifePresentation.pdf>Presentation</a>.
[<a href="http://bicici.github.io/publications/2004/IEEETech4LifeConf2004.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/INDUCTIVE-LOGIC-PROGRAMMING.html">Inductive Logic Programming</a>,
<a href="../Keyword/MACHINE-LEARNING.html">Machine Learning</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
We applied relational learning to discover rules from adverse events reports. We used the FOIL relational learning system to and a set of rules for withdrawn drugs. We compared our results with FDA's reasons for withdrawal.</td>

</tr></table></center>
[bibtex-key = Bicici:DrugAER]
[<a href="../Year/2004.complete.html#Bicici:DrugAER">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2003
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="Bicici:RAFTA"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Robert St. Amant.
<strong>Reasoning About the Functionality of Tools and Physical Artifacts</strong>.
Technical report TR-2003-22,
North Carolina State University,
2003.
Note: <a href=../2003/>Presentation</a>.
[<a href="http://bicici.github.io/publications/2003/ebiciciRAFTPA.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Tool use is an important characteristic of intelligent human behavior. Representing, classifying and recognizing tools by their functionality can provide us new opportunities for understanding and eventually improving an agent's interaction with the physical world. Techniques have been developed in a wide range of areas within artificial intelligence and other disciplines to represent and automatically reason about the functionality of tools. This article surveys past approaches to reasoning about functionality in the literature and attempts to give an overview of the strengths and weaknesses of previous techniques. A number of issues that needs to be addressed are also reviewed.</td>

</tr></table></center>
[bibtex-key = Bicici:RAFTA]
[<a href="../Year/2003.complete.html#Bicici:RAFTA">bibtex-entry</a>]

</li>
<br /><br />


<li>
<a name="IUIT2003"></a>Sameer Rajyaguru,
<a href="../Author/BICICI-E.html">Ergun Biçici</a>,
 and Robert St. Amant.
<strong>Intelligent User Interface Tools</strong>.
Note: MSc Work,
2003.
[<a href="http://bicici.github.io/publications/2003/ebiciciIDA2003.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
This paper describes an analysis tool for dynamic gaming environments. It is based on an image processing system and a contingency analysis tool for extracting the rules that the user is applying in a driving game simulation. The tool is intended for a cognitive modelling architecture that will learn from the user's previous actions to minimize the amount of assumptions made to control the environment. Interaction with the dynamically changing visual environments at real-time is important. This paper discusses our initial attempt to analyze dynamic gaming environments through statistical measures, our learning goal from previous experiences, the limitations, and its implications for user interface environments that dynamically change.</td>

</tr></table></center>
[bibtex-key = IUIT2003]
[<a href="../Year/2003.complete.html#IUIT2003">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2002
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="ProlegomenonCSRUI"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a>.
<strong>Prolegomenon to Commonsense Reasoning in User Interfaces</strong>.
<em>ACM Crossroads</em>,
9(1),
2002.
[<a href="http://bicici.github.io/publications/2002/ACMC/CommonsenseUI6.htm">WWW</a>]
[<a href="http://bicici.github.io/publications/2002/ACMC/Prolegomenon2.pdf">PDF</a>]
Keyword(s): <a href="../Keyword/ARTIFICIAL-INTELLIGENCE.html">Artificial Intelligence</a>.
<center>
<table border=1 align=center width=80%>
<tr>
Abstract: <td>
Human-computer interaction experiences unbalanced talents of counterparts in the user interface, which can only be eased with the introduction of new solutions. Commonsense reasoning is a promising answer that offers formalization and computational models about how humans reason and think in a sensible way. In user interface design, assumed conventions and rules are widely followed and carried to user interfaces. For the most part, these assumptions are obvious to humans yet incomprehensible to computers. As a result, it is essential that tools are developed, capable of retrieving relevant, sensible inferences that in turn can serve as catalysts for future reasoning. Embedding this tool in user interfaces can provide many benefits including representation of assumptions and unspoken rules, the addition of useful tool abilities, and increasingly usable and accessible environments where computers and humans have extended communication capabilities to assist in understanding each other. In this article, we present groundwork for applying commonsense reasoning to user interfaces. We start with identifying the asymmetry in human-machine communication and later focus on some approaches such as softbots and the proposed anti-mac interface. Then we identify the problems faced in user interfaces such as the correspondence problem inherited from computer vision. Next, we state our research ambition as adding commonsense reasoning functionality to user interfaces and further survey previous approaches and the state of the art. We depict the big picture we are facing and list some of the rewards to be earned by applying this technique. Later, characteristics of common sense are investigated together with examples in first-order logic. We then report various lessons learned from earlier attempts that began in physical systems concentrated on small microworld problems. Finally, we cite a sample methodology for automating commonsense reasoning and identify a number of questions that have yet to be answered.</td>

</tr></table></center>
[bibtex-key = ProlegomenonCSRUI]
[<a href="../Year/2002.complete.html#ProlegomenonCSRUI">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<table width="100%">
<tr><td height="35"  align="center" valign="center" bgcolor="#badfe1">
<strong><font size=4 face="times">
2000
</font></strong>
</td></tr>
</table>


<ol>

<li>
<a name="COANTT2000"></a><a href="../Author/BICICI-E.html">Ergun Biçici</a> and Baris Arslan.
<strong>Corpus Annotation Tool for Turkish Language</strong>.
Technical report,
Bilkent University,
2000.
Note: Senior Project under the supervision of Kemal Oflazer.
Keyword(s): <a href="../Keyword/NATURAL-LANGUAGE-PROCESSING.html">Natural Language Processing</a>.
[bibtex-key = COANTT2000]
[<a href="../Year/2000.complete.html#COANTT2000">bibtex-entry</a>]

</li>
<br /><br />


</ol>

<br />
<a href="../index.html"><strong> BACK TO INDEX </strong></a>
<br /><br />


<br /><hr size="2" width="100%"><br />

<u><strong>Disclaimer:</strong></u><br /><br />
<p><em>
This material is presented to ensure timely dissemination of
scholarly and technical work. Copyright and all rights therein
are retained by authors or by other copyright holders.
All person copying this information are expected to adhere to
the terms and constraints invoked by each author's copyright.
In most cases, these works may not be reposted
without the explicit permission of the copyright holder.

</em></p>
<p><em>
Les documents contenus dans ces répertoires sont rendus disponibles
par les auteurs qui y ont contribué en vue d'assurer la diffusion
à temps de travaux savants et techniques sur une base non-commerciale.
Les droits de copie et autres droits sont gardés par les auteurs
et par les détenteurs du copyright, en dépit du fait qu'ils présentent
ici leurs travaux sous forme électronique. Les personnes copiant ces
informations doivent adhérer aux termes et contraintes couverts par
le copyright de chaque auteur. Ces travaux ne peuvent pas être
rendus disponibles ailleurs sans la permission explicite du détenteur
du copyright.

</em></p>

<br /><hr size="2" width="100%"><br />

Last modified: Fri Jun 11 15:57:27 2021

<br />Author: ebicici.

<br /><hr size="2" width="100%"><br />

<p>This document was translated from BibT<sub>E</sub>X by
<a href="http://www-sop.inria.fr/epidaure/personnel/malandain/codes/bibtex2html.html"><em>bibtex2html</em></a>
</p>


</body>


</html>
