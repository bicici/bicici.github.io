


%
%
%
% automatically generated
% % /opt/Programs/bibtex2html-1.03//src/bibtex2html -force -html-links -sort year -icons -copy-icons -create-directories -style html /home/ebicici/Webpage/ebiciciPubs.bib
% Date: Sat Dec 12 19:10:19 2015

% Author: ebicici
%
%
%










@ARTICLE{BiciciPBML2015,
   AUTHOR       = {Ergun Biçici},
   JOURNAL      = {The Prague Bulletin of Mathematical Linguistics},
   OPTMONTH     = {},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   PAGES        = {5--20},
   TITLE        = {Domain Adaptation for Machine Translation with Instance 
      Selection},
   VOLUME       = {103},
   YEAR         = {2015},
   ABSTRACT     = {Domain adaptation for machine translation (MT) can be 
      achieved by selecting training instances close to the test set from a 
      larger set of instances. We consider $7$ different domain adaptation 
      strategies and answer $7$ research questions, which give us a recipe 
      for domain adaptation in MT. We perform English to German statistical 
      MT (SMT) experiments in a setting where test and training sentences 
      can come from different corpora and one of our goals is to learn the 
      parameters of the sampling process. Domain adaptation with training 
      instance selection can obtain $22\%$ increase in target $2$-gram 
      recall and can gain up to $3.55$ BLEU points compared with random 
      selection. Domain adaptation with feature decay algorithm (FDA) not 
      only achieves the highest target $2$-gram recall and BLEU performance 
      but also perfectly learns the test sample distribution parameter with 
      correlation $0.99$. Moses SMT systems built with FDA selected 10K 
      training sentences is able to obtain $F_1$ results as good as the 
      baselines that use up to 2M sentences. Moses SMT systems built with 
      FDA selected 50K training sentences is able to obtain 1 $F_1$ point 
      better results than the baselines.},
   DOI          = {10.1515/pralin-2015-0001},
   OPTISBN      = {},
   ISSN         = {1804-0462},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Domain Adaptation},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@ARTICLE{BiciciQuestPBML2015,
   AUTHOR       = {Ergun Biçici and Lucia Specia},
   JOURNAL      = {The Prague Bulletin of Mathematical Linguistics},
   OPTMONTH     = {},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   PAGES        = {43--64},
   TITLE        = {QuEst for High Quality Machine Translation},
   VOLUME       = {103},
   YEAR         = {2015},
   ABSTRACT     = {In this paper we describe the use of \textsc{QuEst}, a 
      framework that aims to obtain predictions on the quality of 
      translations, to improve the performance of machine translation (MT) 
      systems without changing their internal functioning. We apply 
      \textsc{QuEst} to experiments with: \begin{enumerate}[label=\roman*.] 
      \item multiple system translation ranking, where translations 
      produced by different MT systems are ranked according to their 
      estimated quality, leading to gains of up to $2.72$ BLEU, $3.66$ 
      BLEUs, and $2.17$ $F_1$ points; \item n-best list re-ranking, where 
      n-best list translations produced by an MT system are re-ranked based 
      on predicted quality scores to get the best translation ranked top, 
      which lead to improvements on sentence NIST score by $0.41$ points; 
      \item n-best list combination, where segments from an n-best list are 
      combined using a lattice-based re-scoring approach that minimize word 
      error, obtaining gains of $0.28$ BLEU points; and \item the ITERPE 
      strategy, which attempts to identify translation errors regardless of 
      prediction errors (ITERPE) and build sentence-specific SMT systems 
      (SSSS) on the ITERPE sorted instances identified as having more 
      potential for improvement, achieving gains of up to $1.43$ BLEU, 
      $0.54$ $F_1$, $2.9$ NIST, $0.64$ sentence BLEU, and $4.7$ sentence 
      NIST points in English to German over the top $100$ ITERPE sorted 
      instances. \end{enumerate}},
   DOI          = {10.1515/pralin-2015-0003},
   OPTISBN      = {},
   ISSN         = {1804-0462},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@ARTICLE{Bicici:RTM_SEMEVAL,
   AUTHOR       = {Ergun Biçici and Andy Way},
   JOURNAL      = {Language Resources and Evaluation},
   OPTMONTH     = {},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   PAGES        = {1-27},
   PUBLISHER    = {Springer Netherlands},
   TITLE        = {Referential translation machines for predicting semantic 
      similarity},
   OPTVOLUME    = {},
   YEAR         = {2015},
   ABSTRACT     = {Referential translation machines (RTMs) are a 
      computational model effective at judging monolingual and bilingual 
      similarity while identifying translation acts between any two data 
      sets with respect to interpretants. RTMs pioneer a 
      language-independent approach to all similarity tasks and remove the 
      need to access any task or domain-specific information or resource. 
      We use RTMs for predicting the semantic similarity of text and 
      present state-of-the-art results showing that RTMs can achieve better 
      results on the test set than on the training set. RTMs judge the 
      quality or the semantic similarity of texts by using relevant 
      retrieved training data as interpretants for reaching shared 
      semantics. Interpretants are used to derive features measuring the 
      closeness of the test sentences to the training data, the difficulty 
      of translating them, and the presence of the acts of translation, 
      which may ubiquitously be observed in communication. RTMs achieve top 
      performance at SemEval in various semantic similarity prediction 
      tasks as well as similarity prediction tasks in bilingual settings. 
      We define MAER, mean absolute error relative to the magnitude of the 
      target, and MRAER, mean absolute error relative to the absolute error 
      of a predictor always predicting the target mean assuming that target 
      mean is known. RTM test performance on various tasks sorted according 
      to MRAER can help identify which tasks and subtasks require more work 
      by design.},
   DOI          = {10.1007/s10579-015-9322-7},
   OPTISBN      = {},
   ISSN         = {1574-020X},
   KEYWORDS     = {Referential translation machine, RTM, 
      Semantic similarity, Machine translation, Performance prediction, 
      Machine translation performance prediction},
   URL          = {http://dx.doi.org/10.1007/s10579-015-9322-7},
   OPTURL-PUBLISHER = {}
}

@ARTICLE{BiciciYuret:FDA5:TASLP,
   AUTHOR       = {Ergun Biçici and Deniz Yuret},
   JOURNAL      = {IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP)},
   OPTMONTH     = {},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   PAGES        = {339-350},
   TITLE        = {Optimizing Instance Selection for Statistical Machine 
      Translation with Feature Decay Algorithms},
   VOLUME       = {23},
   YEAR         = {2015},
   ABSTRACT     = {We introduce FDA5 for efficient parameterization, 
      optimization, and implementation of feature decay algorithms (FDA), a 
      class of instance selection algorithms that use feature decay. FDA 
      increase the diversity of the selected training set by devaluing 
      features (i.e. n-grams) that have already been included. FDA5 decides 
      which instances to select based on three functions used for 
      initializing and decaying feature values and scaling sentence scores 
      controlled with $5$ parameters. We present optimization techniques 
      that allow FDA5 to adapt these functions to in-domain and 
      out-of-domain translation tasks for different language pairs. In a 
      transductive learning setting, selection of training instances 
      relevant to the test set can improve the final translation quality. 
      In machine translation experiments performed on the $2$ million 
      sentence English-German section of the Europarl corpus, we show that 
      a subset of the training set selected by FDA5 can gain up to $3.22$ 
      BLEU points compared to a randomly selected subset of the same size, 
      can gain up to $0.41$ BLEU points compared to using all of the 
      available training data using only $15\%$ of it, and can reach within 
      $0.5$ BLEU points to the full training set result by using only 
      $2.7\%$ of the full training data. FDA5 peaks at around 8M words or 
      $15\%$ of the full training set. In an active learning setting, FDA5 
      minimizes the human effort by identifying the most informative 
      sentences for translation and FDA gains up to $0.45$ BLEU points 
      using $3/5$ of the available training data compared to using all of 
      it and $1.12$ BLEU points compared to random training set. In 
      translation tasks involving English and Turkish, a morphologically 
      rich language, FDA5 can gain up to $11.52$ BLEU points compared to a 
      randomly selected subset of the same size, can achieve the same BLEU 
      score using as little as $4\%$ of the data compared to random 
      instance selection, and can exceed the full dataset result by $0.78$ 
      BLEU points. FDA5 is able to reduce the time to build a statistical 
      machine translation system to about half with 1M words using only 
      $3\%$ of the space for the phrase table and $8\%$ of the overall 
      space when compared with a baseline system using all of the training 
      data available yet still obtain only $0.58$ BLEU points difference 
      with the baseline system in out-of-domain translation.},
   DOI          = {10.1109/TASLP.2014.2381882},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Artificial Intelligence, Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici2015_SEMEVAL_SS,
   ADDRESS      = {Denver, Colorado, USA},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {SemEval-2015: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation},
   OPTEDITOR    = {},
   MONTH        = {June},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {RTM-DCU: Predicting Semantic Similarity with Referential 
      Translation Machines},
   OPTVOLUME    = {},
   YEAR         = {2015},
   ABSTRACT     = {We use referential translation machines (RTMs) for 
      predicting the semantic similarity of text. RTMs are a computational 
      model effectively judging monolingual and bilingual similarity while 
      identifying translation acts between any two data sets with respect 
      to interpretants. RTMs pioneer a language independent approach to all 
      similarity tasks and remove the need to access any task or domain 
      specific information or resource. RTMs become the 2nd system out of 
      13 systems participating in Paraphrase and Semantic Similarity in 
      Twitter, 6th out of 16 submissions in Semantic Textual Similarity 
      Spanish, and 50th out of 73 submissions in Semantic Textual 
      Similarity English.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction, Semantic Similarity},
   URL          = {http://aclanthology.info/papers/rtm-dcu-predicting-semantic-similarity-with-referential-translation-machines},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici:FDA54FDA:WMT15,
   ADDRESS      = {Lisbon, Portugal},
   AUTHOR       = {Ergun Biçici and Qun Liu and Andy Way},
   BOOKTITLE    = {Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {September},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {ParFDA for Fast Deployment of Accurate Statistical 
      Machine Translation Systems, Benchmarks, and Statistics},
   OPTVOLUME    = {},
   YEAR         = {2015},
   ABSTRACT     = {We build parallel FDA5 (ParFDA) Moses statistical 
      machine translation (SMT) systems for all language pairs in the 
      workshop on statistical machine translation~\cite{WMT2015} (WMT15) 
      translation task and obtain results close to the top with an average 
      of $3.176$ BLEU points difference using significantly less resources 
      for building SMT systems. ParFDA is a parallel implementation of 
      feature decay algorithms (FDA) developed for fast deployment of 
      accurate SMT systems. ParFDA Moses SMT system we built is able to 
      obtain the top TER performance in French to English translation. We 
      make the data for building ParFDA Moses SMT systems for WMT15 
      available: \url{https://github.com/bicici/ParFDAWMT15}.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Language Modeling},
   URL          = {http://aclanthology.info/papers/parfda-for-fast-deployment-of-accurate-statistical-machine-translation-systems-benchmarks-and-statistics},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici:RTM4QE:WMT15,
   ADDRESS      = {Lisbon, Portugal},
   AUTHOR       = {Ergun Biçici and Qun Liu and Andy Way},
   BOOKTITLE    = {Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {September},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Referential Translation Machines for Predicting 
      Translation Quality and Related Statistics},
   OPTVOLUME    = {},
   YEAR         = {2015},
   ABSTRACT     = {We use referential translation machines (RTMs) for 
      predicting translation performance. RTMs pioneer a language 
      independent approach to all similarity tasks and remove the need to 
      access any task or domain specific information or resource. We 
      improve our RTM models with the ParFDA instance selection 
      model~\cite{Bicici:FDA54FDA:WMT15}, with additional features for 
      predicting the translation performance, and with improved learning 
      models. We develop RTM models for each WMT15 QET (QET15) subtask and 
      obtain improvements over QET14 results. RTMs achieve top performance 
      in QET15 ranking $1$st in document- and sentence-level prediction 
      tasks and $2$nd in word-level prediction task.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction},
   URL          = {http://aclanthology.info/papers/referential-translation-machines-for-predicting-translation-quality-and-related-statistics},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici:FDA54FDA:WMT14,
   ADDRESS      = {Baltimore, USA},
   AUTHOR       = {Ergun Biçici and Qun Liu and Andy Way},
   BOOKTITLE    = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {June},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Parallel FDA5 for Fast Deployment of Accurate 
      Statistical Machine Translation Systems},
   OPTVOLUME    = {},
   YEAR         = {2014},
   ABSTRACT     = {We use parallel FDA5, an efficiently parameterized and 
      optimized parallel implementation of feature decay algorithms for 
      fast deployment of accurate statistical machine translation systems, 
      taking only about half a day for each translation direction. We build 
      Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 
      translation task and obtain SMT performance close to the top Moses 
      systems with an average of $3.49$ BLEU points difference using 
      significantly less resources for training and development.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Language Modeling},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2014/ParFDA5forFDASMT.pdf}
}

@INPROCEEDINGS{Bicici2014_SEMEVAL_SS,
   ADDRESS      = {Dublin, Ireland},
   AUTHOR       = {Ergun Biçici and Andy Way},
   BOOKTITLE    = {SemEval-2014: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation},
   OPTEDITOR    = {},
   MONTH        = {23-24 August},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {RTM-DCU: Referential Translation Machines for Semantic 
      Similarity},
   OPTVOLUME    = {},
   YEAR         = {2014},
   ABSTRACT     = {We use referential translation machines (RTMs) for 
      predicting the semantic similarity of text. RTMs are a computational 
      model for identifying the translation acts between any two data sets 
      with respect to interpretants selected in the same domain, which are 
      effective when making monolingual and bilingual similarity judgments. 
      RTMs judge the quality or the semantic similarity of text by using 
      retrieved relevant training data as interpretants for reaching shared 
      semantics. We derive features measuring the closeness of the test 
      sentences to the training data via interpretants, the difficulty of 
      translating them, and the presence of the acts of translation, which 
      may ubiquitously be observed in communication. RTMs provide a 
      language independent solution to all similarity tasks and achieve top 
      performance when predicting monolingual cross-level semantic 
      similarity (Task 3) and good results in the semantic relatedness and 
      entailment (Task 1) and multilingual semantic textual similarity 
      (STS) (Task 10). RTMs remove the need to access any task or domain 
      specific information or resource.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction, Semantic Similarity},
   URL          = {http://aclanthology.info/papers/rtm-dcu-referential-translation-machines-for-semantic-similarity},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2014/RTM_STS.pdf}
}

@INPROCEEDINGS{Bicici:RTM4QE:WMT14,
   ADDRESS      = {Baltimore, USA},
   AUTHOR       = {Ergun Biçici and Andy Way},
   BOOKTITLE    = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {June},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Referential Translation Machines for Predicting 
      Translation Quality},
   OPTVOLUME    = {},
   YEAR         = {2014},
   ABSTRACT     = {We use referential translation machines (RTM) for 
      quality estimation of translation outputs. RTMs are a computational 
      model for identifying the translation acts between any two data sets 
      with respect to interpretants selected in the same domain, which are 
      effective when making monolingual and bilingual similarity judgments. 
      RTMs achieve top performance in automatic, accurate, and language 
      independent prediction of sentence-level and word-level statistical 
      machine translation (SMT) quality. RTMs remove the need to access any 
      SMT system specific information or prior knowledge of the training 
      data or models used when generating the translations and achieve the 
      top performance in WMT13 quality estimation task (QET13). We improve 
      our RTM models with the Parallel FDA5 instance selection model, with 
      additional features for predicting the translation performance, and 
      with improved learning models. We develop RTM models for each WMT14 
      QET (QET14) subtask, obtain improvements over QET13 results, and rank 
      $1$st in all of the tasks and subtasks of QET14.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction, Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2014/RTMforQE.pdf}
}

@TECHREPORT{QTLPD2.2.2,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   INSTITUTION  = {Dublin City University},
   OPTMONTH     = {},
   NOTE         = {Contribution to EU project QTLaunchPad deliverable D2.2.2: http://www.qt21.eu/launchpad/content/delivered},
   OPTNUMBER    = {},
   TITLE        = {Quality Estimation for Extending Good Translations},
   OPTTYPE      = {},
   YEAR         = {2014},
   ABSTRACT     = {We present experiments using quality estimation models 
      to improve the performance of statistical machine translation (SMT) 
      systems by supplementing their training corpora or by building 
      sentence-specific SMT models for instances predicted as having 
      potential for improvement by the ITERPE model. The experiments with 
      quality-informed active learning strategy select, among alternative 
      machine translations, those which are: (i) predicted to have high 
      quality, and thus can be added to the machine translation system 
      training set; (ii) predicted to have low quality, and thus need to be 
      corrected/translated by humans, with the human corrections added to 
      the machine translation system training set. Improvement is measured 
      by the increase in the performance of the overall machine translation 
      systems on held-out datasets, where performance is measured by 
      automatic evaluation metrics comparing the scores of the original 
      machine translation system against the score of the improved machine 
      translation system after additional material is used. The experiments 
      with ITERPE consist in automatically grouping translation instances 
      into different quality bands, for instance for re-translation or for 
      post-editing [Bicici and Specia, 2014]. This method can be helpful in 
      automatic identification of quality barriers in MT to achieve high 
      quality machine translation.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Performance Prediction},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@TECHREPORT{QTLPD2.2.1,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   INSTITUTION  = {Dublin City University},
   OPTMONTH     = {},
   NOTE         = {Contribution to EU project QTLaunchPad deliverable D2.2.1: http://www.qt21.eu/launchpad/content/delivered},
   OPTNUMBER    = {},
   TITLE        = {Quality Estimation for System Selection and Combination},
   OPTTYPE      = {},
   YEAR         = {2014},
   ABSTRACT     = {We present experiments using state of the art quality 
      estimation models to improve the performance of machine translation 
      systems without changing the internal functioning of such systems. 
      The experiments include the following approaches: (i) n-best list 
      re-ranking, where translation candidates (segments) produced by a 
      machine translation system are re-ranked based on predicted quality 
      scores such as to get the best translation ranked top; (ii) n-best 
      list recombination, where sub-segments from the n-best list are mixed 
      using a lattice-based approach, and the complete generated segments 
      are scored using quality predictions and then re-ranked as in (i); 
      (iii) system selection, where translations produced by multiple 
      machine translation systems and a human translator are sorted 
      according to predicted quality to select the best translated segment, 
      including the challenging case where the source of the translation 
      (i.e., which system/human produced it) is unknown, and (iv) diagnosis 
      of statistical machine translation systems by looking at internal 
      features of the decoder and their correlation with translation 
      quality, as well as using them to predict groups of errors in the 
      translations.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Performance Prediction},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@PATENT{BiciciIDF_ITERPE2014,
   ADDRESS      = {},
   AUTHOR       = {Ergun Biçici},
   NOTE         = {Invention Disclosure, DCU Invent Innovation and Enterprise: https://www.dcu.ie/invent/},
   NUMBER       = {},
   TITLE        = {ITERPE: Identifying Translation Errors Regardless of 
      Prediction Errors},
   YEAR         = {2014},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   OPTKEYWORDS  = {},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@ARTICLE{BiciciMTPPMTJ,
   AUTHOR       = {Ergun Biçici and Declan Groves and van Genabith, Josef},
   JOURNAL      = {Machine Translation},
   OPTMONTH     = {},
   OPTNOTE      = {},
   NUMBER       = {3-4},
   PAGES        = {171-192},
   TITLE        = {Predicting Sentence Translation Quality Using Extrinsic 
      and Language Independent Features},
   VOLUME       = {27},
   YEAR         = {2013},
   ABSTRACT     = {We develop a top performing model for automatic, 
      accurate, and language independent prediction of sentence-level 
      statistical machine translation (SMT) quality with or without looking 
      at the translation outputs. We derive various feature functions 
      measuring the closeness of a given test sentence to the training data 
      and the difficulty of translating the sentence. We describe 
      \texttt{mono} feature functions that are based on statistics of only 
      one side of the parallel training corpora and \texttt{duo} feature 
      functions that incorporate statistics involving both source and 
      target sides of the training data. Overall, we describe novel, 
      language independent, and SMT system extrinsic features for 
      predicting the SMT performance, which also rank high during feature 
      ranking evaluations. We experiment with different learning settings, 
      with or without looking at the translations, which help differentiate 
      the contribution of different feature sets. We apply partial least 
      squares and feature subset selection, both of which improve the 
      results and we present ranking of the top features selected for each 
      learning setting, providing an exhaustive analysis of the extrinsic 
      features used. We show that by just looking at the test source 
      sentences and not using the translation outputs at all, we can 
      achieve better performance than a baseline system using SMT model 
      dependent features that generated the translations. Furthermore, our 
      prediction system is able to achieve the $2$nd best performance 
      overall according to the official results of the Quality Estimation 
      Task (QET) challenge when also looking at the translation outputs. 
      Our representation and features achieve the top performance in QET 
      among the models using the SVR learning model.},
   DOI          = {10.1007/s10590-013-9138-4},
   OPTISBN      = {},
   ISSN         = {0922-6567},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction},
   URL          = {http://dx.doi.org/10.1007/s10590-013-9138-4},
   OPTURL-PUBLISHER = {}
}

@ARTICLE{BiciciPBML2013,
   AUTHOR       = {Kashif Shah and Eleftherios Avramidis and Ergun Biçici and 
      Lucia Specia},
   JOURNAL      = {The Prague Bulletin of Mathematical Linguistics},
   OPTMONTH     = {},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   PAGES        = {19--30},
   TITLE        = {QuEst - Design, Implementation and Extensions of a 
      Framework for Machine Translation Quality Estimation},
   VOLUME       = {100},
   YEAR         = {2013},
   OPTABSTRACT  = {},
   DOI          = {10.2478/pralin-2013-0008},
   OPTISBN      = {},
   OPTISSN      = {},
   OPTKEYWORDS  = {},
   URL          = {http://ufal.mff.cuni.cz/pbml/100/art-shah-avramidis-bicici-specia.pdf},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici:FDA4FDA:WMT13,
   ADDRESS      = {Sofia, Bulgaria},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {August},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Feature Decay Algorithms for Fast Deployment of Accurate 
      Statistical Machine Translation Systems},
   OPTVOLUME    = {},
   YEAR         = {2013},
   ABSTRACT     = {We use feature decay algorithms (FDA) for fast 
      deployment of accurate statistical machine translation systems taking 
      only about half a day for each translation direction. We develop 
      parallel FDA for solving computational scalability problems caused by 
      the abundance of training data for SMT models and language models and 
      still achieve SMT performance that is on par with using all of the 
      training data or better. Parallel FDA runs separate FDA models on 
      randomized subsets of the training data and combines the instance 
      selections later. Parallel FDA can also be used for selecting the LM 
      corpus based on the training set selected by parallel FDA. The high 
      quality of the selected training data allows us to obtain very 
      accurate translation outputs close to the top performing SMT systems. 
      The relevancy of the selected LM corpus can reach up to $86\%$ 
      reduction in the number of OOV tokens and up to $74\%$ reduction in 
      the perplexity. We perform SMT experiments in all language pairs in 
      the WMT13 translation task and obtain SMT performance close to the 
      top systems using significantly less resources for training and 
      development.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Language Modeling},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2013/FDAforFDA.pdf}
}

@INPROCEEDINGS{Bicici:RTM4QE:WMT13,
   ADDRESS      = {Sofia, Bulgaria},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {August},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Referential Translation Machines for Quality Estimation},
   OPTVOLUME    = {},
   YEAR         = {2013},
   ABSTRACT     = {We introduce referential translation machines (RTM) for 
      quality estimation of translation outputs. RTMs are a computational 
      model for identifying the translation acts between any two data sets 
      with respect to a reference corpus selected in the same domain, which 
      can be used for estimating the quality of translation outputs, 
      judging the semantic similarity between text, and evaluating the 
      quality of student answers. RTMs achieve top performance in 
      automatic, accurate, and language independent prediction of 
      sentence-level and word-level statistical machine translation (SMT) 
      quality. RTMs remove the need to access any SMT system specific 
      information or prior knowledge of the training data or models used 
      when generating the translations. We develop novel techniques for 
      solving all subtasks in the WMT13 quality estimation (QE) task (QET 
      2013) based on individual RTM models. Our results achieve 
      improvements over last year's QE task results (QET 2012), as well as 
      our previous results, provide new features and techniques for QE, and 
      rank $1$st or $2$nd in all of the subtasks.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction, Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2013/RTMforQE.pdf}
}

@INPROCEEDINGS{Bicici2013_STARSEM_STS,
   ADDRESS      = {Atlanta, Georgia, USA},
   AUTHOR       = {Ergun Biçici and van Genabith, Josef},
   BOOKTITLE    = {*SEM 2013: The Second Joint Conference on Lexical and Computational Semantics},
   OPTEDITOR    = {},
   MONTH        = {13-14 June},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {CNGL-CORE: Referential Translation Machines for 
      Measuring Semantic Similarity},
   OPTVOLUME    = {},
   YEAR         = {2013},
   ABSTRACT     = {We invent referential translation machines (RTMs), a 
      computational model for identifying the translation acts between any 
      two data sets with respect to a reference corpus selected in the same 
      domain, which can be used for judging the semantic similarity between 
      text. RTMs make quality and semantic similarity judgments possible by 
      using retrieved relevant training data as interpretants for reaching 
      shared semantics. An MTPP (machine translation performance predictor) 
      model derives features measuring the closeness of the test sentences 
      to the training data, the difficulty of translating them, and the 
      presence of acts of translation involved. We view semantic similarity 
      as paraphrasing between any two given texts. Each view is modeled by 
      an RTM model, giving us a new perspective on the binary relationship 
      between the two. Our prediction model is the $15$th on some tasks and 
      $30$th overall out of $89$ submissions in total according to the 
      official results of the Semantic Textual Similarity (STS 2013) 
      challenge.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction, Natural Language Processing, 
      Artificial Intelligence},
   URL          = {http://aclweb.org/anthology/S/S13/S13-1034.pdf},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2013/STS_RTM.pdf}
}

@INPROCEEDINGS{Bicici2013_STARSEM_SRA,
   ADDRESS      = {Atlanta, Georgia, USA},
   AUTHOR       = {Ergun Biçici and van Genabith, Josef},
   BOOKTITLE    = {*SEM 2013: The Second Joint Conference on Lexical and Computational Semantics and Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)},
   OPTEDITOR    = {},
   MONTH        = {14-15 June},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {CNGL: Grading Student Answers by Acts of Translation},
   OPTVOLUME    = {},
   YEAR         = {2013},
   ABSTRACT     = {We invent referential translation machines (RTMs), a 
      computational model for identifying the translation acts between any 
      two data sets with respect to a reference corpus selected in the same 
      domain, which can be used for automatically grading student answers. 
      RTMs make quality and semantic similarity judgments possible by using 
      retrieved relevant training data as interpretants for reaching shared 
      semantics. An MTPP (machine translation performance predictor) model 
      derives features measuring the closeness of the test sentences to the 
      training data, the difficulty of translating them, and the presence 
      of acts of translation involved. We view question answering as 
      translation from the question to the answer, from the question to the 
      reference answer, from the answer to the reference answer, or from 
      the question and the answer to the reference answer. Each view is 
      modeled by an RTM model, giving us a new perspective on the ternary 
      relationship between the question, the answer, and the reference 
      answer. We show that all RTM models contribute and a prediction model 
      based on all four perspectives performs the best. Our prediction 
      model is the $2$nd best system on some tasks according to the 
      official results of the Student Response Analysis (SRA 2013) 
      challenge.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Performance Prediction, Natural Language Processing},
   URL          = {http://aclweb.org/anthology/S/S13/S13-2098.pdf},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2013/SRA_AOT.pdf}
}

@TECHREPORT{QTLPD3.2.1,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   INSTITUTION  = {Dublin City University},
   OPTMONTH     = {},
   NOTE         = {Contribution to EU project QTLaunchPad deliverable D3.2.1: http://www.qt21.eu/launchpad/content/delivered},
   OPTNUMBER    = {},
   TITLE        = {Definition of Interfaces},
   OPTTYPE      = {},
   YEAR         = {2013},
   ABSTRACT     = {The aim of this report is to define the interfaces for 
      the tools used in the MT development and evaluation scenarios as 
      included in the QTLaunchPad (QTLP) infrastructure. Specification of 
      the interfaces is important for the interaction and interoperability 
      of the tools in the developed QTLP infrastructure. In addressing this 
      aim, the report provides: 1. Descriptions of the common aspects of 
      the tools and their standardized data formatsÍ¾ 2. Descriptions of 
      the interfaces for the tools for interoperability. where the tools 
      are categorized into preparation, development, and evaluation 
      categories including the human interfaces for quality assessment with 
      multidimensional quality metrics. Interface specifications allow a 
      modular tool infrastructure, flexibly selecting among alternative 
      implementations, enabling realistic expectations to be made at 
      different sections of the QTLP information flow pipeline, and 
      supporting the QTLP infrastructure. D3.2.1 allows the emergence of 
      the QTLP infrastructure and helps the identification and acquisition 
      of existing tools (D4.4.1), the integration of identified language 
      processing tools (D3.3.1), their implementation (D3.4.1), and their 
      testing (D3.5.1). QTLP infrastructure will facilitate the 
      organization and running of the quality translation shared task 
      (D5.2.1). We also provide human interfaces for translation quality 
      assessment with the multidimensional quality metrics (D1.1.1). D3.2.1 
      is a living document until M12, which is when the identification and 
      acquisition of existing tools (D4.4.1) and the implementation of 
      identified language processing tools (D3.4.1) are due.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@TECHREPORT{QTLPD3.1.1,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   INSTITUTION  = {Dublin City University},
   OPTMONTH     = {},
   NOTE         = {Contribution to EU project QTLaunchPad deliverable D3.1.1: http://www.qt21.eu/launchpad/content/delivered},
   OPTNUMBER    = {},
   TITLE        = {Definition of Machine Translation and Evaluation 
      Workflows},
   OPTTYPE      = {},
   YEAR         = {2013},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@TECHREPORT{QTLPD2.1.3,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   INSTITUTION  = {Dublin City University},
   OPTMONTH     = {},
   NOTE         = {Contribution to EU project QTLaunchPad deliverable D2.1.3: http://www.qt21.eu/launchpad/content/delivered},
   OPTNUMBER    = {},
   TITLE        = {Quality Estimation for Dissemination},
   OPTTYPE      = {},
   YEAR         = {2013},
   ABSTRACT     = {We present benchmarking experiments for the intrinsic 
      and extrinsic evaluation of an extended version of our open source 
      framework for machine translation quality estimation QUEST, which is 
      described in D2.1.2. We focus on the application of quality 
      predictions for dissemination by estimating post-editing effort. As 
      an extrinsic task, we use quality predictions to rank alternative 
      translations from multiple MT systems according to their estimated 
      quality. Additionally, we experiment with a small dataset annotated 
      for quality labels with different levels of granularity in a attempt 
      to predict multidimensional quality metric (MQM) scores.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Performance Prediction},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@PATENT{BiciciIDF_RTM2013,
   ADDRESS      = {},
   AUTHOR       = {Ergun Biçici},
   NOTE         = {Invention Disclosure, DCU Invent Innovation and Enterprise: https://www.dcu.ie/invent/},
   NUMBER       = {},
   TITLE        = {Referential Translation Machines},
   YEAR         = {2013},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   OPTKEYWORDS  = {},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@PATENT{BiciciIDF_MTPP2012,
   ADDRESS      = {},
   AUTHOR       = {Ergun Biçici},
   NOTE         = {Invention Disclosure, DCU Invent Innovation and Enterprise: https://www.dcu.ie/invent/},
   NUMBER       = {},
   TITLE        = {Machine Translation Performance Predictor},
   YEAR         = {2012},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   OPTKEYWORDS  = {},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@PHDTHESIS{BiciciThesis,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   OPTMONTH     = {},
   NOTE         = {Supervisor: Deniz Yuret},
   SCHOOL       = {Koç University},
   TITLE        = {The Regression Model of Machine Translation},
   OPTTYPE      = {},
   YEAR         = {2011},
   ABSTRACT     = {Machine translation is the task of automatically finding 
      the translation of a source sentence in the target language. 
      Statistical machine translation (SMT) use parallel corpora or 
      bilingual paired corpora that are known to be translations of each 
      other to find a likely translation for a given source sentence based 
      on the observed translations. The task of machine translation can be 
      seen as an instance of estimating the functions that map strings to 
      strings. Regression based machine translation (RegMT) approach 
      provides a learning framework for machine translation, separating 
      learning models for training, training instance selection, feature 
      representation, and decoding. We use the transductive learning 
      framework for making the RegMT approach computationally more scalable 
      and consider the model building step independently for each test 
      sentence. We develop training instance selection algorithms that not 
      only make RegMT computationally more scalable but also improve the 
      performance of standard SMT systems. We develop better training 
      instance selection techniques than previous work from given parallel 
      training sentences for achieving more accurate RegMT models using 
      less training instances. We introduce L1 regularized regression as a 
      better model than L2 regularized regression for statistical machine 
      translation. Our results demonstrate that sparse regression models 
      are better than L2 regularized regression for statistical machine 
      translation in predicting target features, estimating word 
      alignments, creating phrase tables, and generating translation 
      outputs. We develop good evaluation techniques for measuring the 
      performance of the RegMT model and the quality of the translations. 
      We use F1 measure, which performs good when evaluating translations 
      into English according to human judgments. F1 allows us to evaluate 
      the performance of the RegMT models using the target feature 
      prediction vectors or the coefficients matrices learned or a given 
      SMT model using its phrase table without performing the decoding 
      step, which can be computationally expensive. Decoding is dependent 
      on the representation of the training set and the features used. We 
      use graph decoding on the prediction vectors represented in $n$-gram 
      or word sequence counts space found in the training set. We also 
      decode using Moses after transforming the learned weight matrix 
      representing the mappings between the source and target features to a 
      phrase table that can be used by Moses during decoding. We 
      demonstrate that sparse L1 regularized regression performs better 
      than L2 regularized regression in the German-English translation task 
      and in the Spanish-English translation task when using small sized 
      training sets. Graph based decoding can provide an alternative to 
      phrase-based decoding in translation domains having low vocabulary.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning, 
      Artificial Intelligence, Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2011/RegMTThesis_Web.pdf}
}

@INPROCEEDINGS{BiciciYuret:ISforMT:WMT11,
   ADDRESS      = {Edinburgh, Scotland},
   AUTHOR       = {Ergun Bicici and Deniz Yuret},
   BOOKTITLE    = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {July},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {272--283},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Instance Selection for Machine Translation using Feature 
      Decay Algorithms},
   OPTVOLUME    = {},
   YEAR         = {2011},
   ABSTRACT     = {We present an empirical study of instance selection 
      techniques for machine translation. In an active learning setting, 
      instance selection minimizes the human effort by identifying the most 
      informative sentences for translation. In a transductive learning 
      setting, selection of training instances relevant to the test set 
      improves the final translation quality. After reviewing the state of 
      the art in the field, we generalize the main ideas in a class of 
      instance selection algorithms that use feature decay. Feature decay 
      algorithms increase diversity of the training set by devaluing 
      features that are already included. We show that the feature decay 
      rate has a very strong effect on the final translation quality 
      whereas the initial feature values, inclusion of higher order 
      features, or sentence length normalizations do not. We evaluate the 
      best instance selection methods using a standard Moses baseline using 
      the whole 1.6 million sentence English-German section of the Europarl 
      corpus. We show that selecting the best 3000 training sentences for a 
      specific test sentence is sufficient to obtain a score within 1 BLEU 
      of the baseline, using 5\% of the training data is sufficient to 
      exceed the baseline, and a ~2 BLEU improvement over the baseline is 
      possible by optimally selected subset of the training data. In 
      out-of-domain translation, we are able to reduce the training set 
      size to about 7\% and achieve a similar performance with the 
      baseline.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning},
   URL          = {http://www.aclweb.org/anthology/W11-2131},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2011/ISforMTFDA.pdf}
}

@INPROCEEDINGS{BiciciYuret:RegMT:WMT11,
   ADDRESS      = {Edinburgh, Scotland},
   AUTHOR       = {Ergun Bicici and Deniz Yuret},
   BOOKTITLE    = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
   OPTEDITOR    = {},
   MONTH        = {July},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {323--329},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {RegMT System for Machine Translation, System 
      Combination, and Evaluation},
   OPTVOLUME    = {},
   YEAR         = {2011},
   ABSTRACT     = {We present the results we obtain using our RegMT system, 
      which uses transductive regression techniques to learn mappings 
      between source and target features of given parallel corpora and use 
      these mappings to generate machine translation outputs. We present 
      results with our instance selection methods that perform feature 
      decay for proper selection of training instances, which plays an 
      important role to learn correct feature mappings. RegMT uses L2 
      regularized regression as well as L1 regularized regression for 
      sparse regression estimation of target features.We present 
      translation results using our training instance selection methods, 
      translation results using graph decoding, system combination results 
      with RegMT, and performance evaluation with F1 measure over target 
      features.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning},
   URL          = {http://www.aclweb.org/anthology/W11-2137},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2011/RegMTSystem.pdf}
}

@INPROCEEDINGS{BiciciKozat2010:WMT,
   ADDRESS      = {Uppsala, Sweden},
   AUTHOR       = {Ergun Bicici and S. Serdar Kozat},
   BOOKTITLE    = {Proceedings of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR},
   OPTEDITOR    = {},
   MONTH        = {July},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Adaptive Model Weighting and Transductive Regression for 
      Predicting Best System Combinations},
   OPTVOLUME    = {},
   YEAR         = {2010},
   ABSTRACT     = {We analyze adaptive model weighting techniques for 
      reranking using instance scores obtained by L1 regularized 
      transductive regression. Competitive statistical machine translation 
      is an on-line learning technique for sequential translation tasks 
      where we try to select the best among competing statistical machine 
      translators. The competitive predictor assigns a probability per 
      model weighted by the sequential performance. We define additive, 
      multiplicative, and loss-based weight updates with exponential loss 
      functions for competitive statistical machine translation. Without 
      any pre-knowledge of the performance of the translation models, we 
      succeed in achieving the performance of the best model in all systems 
      and surpass their performance in most of the language pairs we 
      considered.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning},
   URL          = {http://www.aclweb.org/anthology/W/W10/W10-1740.pdf},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2010/WMT2010_TRBMT_CSMT.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2010/WMT2010_TRBMT_CSMT.ps}
}

@INPROCEEDINGS{BiciciYuret2010:CSW,
   ADDRESS      = {Koc University, Istinye Campus, Istanbul, Turkey},
   AUTHOR       = {Ergun Bicici and Deniz Yuret},
   BOOKTITLE    = {Proceedings of the Computer Science Student Workshop, CSW'10},
   OPTEDITOR    = {},
   MONTH        = {February},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {L1 Regularization for Learning Word Alignments in Sparse 
      Feature Matrices},
   OPTVOLUME    = {},
   YEAR         = {2010},
   ABSTRACT     = {Sparse feature representations can be used in various 
      domains. We compare the effectiveness of L1 regularization techniques 
      for regression to learn mappings between features given in a sparse 
      feature matrix. We apply these techniques for learning word 
      alignments commonly used for machine translation. The performance of 
      the learned mappings are measured using the phrase table generated on 
      a larger corpus by a state of the art word aligner. The results show 
      the effectiveness of using L1 regularization versus L2 used in ridge 
      regression.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning},
   URL          = {URL: http://myweb.sabanciuniv.edu/csw/},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2010/L1forWordAlignment.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2010/L1forWordAlignment.ps}
}

@INPROCEEDINGS{BiciciYuret2010:WMT,
   ADDRESS      = {Uppsala, Sweden},
   AUTHOR       = {Ergun Bicici and Deniz Yuret},
   BOOKTITLE    = {Proceedings of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR},
   OPTEDITOR    = {},
   MONTH        = {July},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {L1 Regularized Regression for Reranking and System 
      Combination in Machine Translation},
   OPTVOLUME    = {},
   YEAR         = {2010},
   ABSTRACT     = {We use L1 regularized transductive regression to learn 
      mappings between source and target features of the training sets 
      derived for each test sentence and use these mappings to rerank 
      translation outputs. We compare the effectiveness of L1 
      regularization techniques for regression to learn mappings between 
      features given in a sparse feature matrix. The results show the 
      effectiveness of using L1 regularization versus L2 used in ridge 
      regression. We show that regression mapping is effective in reranking 
      translation outputs and in selecting the best system combinations 
      with encouraging results on different language pairs.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Machine Translation, Machine Learning},
   URL          = {http://www.aclweb.org/anthology/W/W10/W10-1741.pdf},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2010/WMT2010_L1forSMTReranking.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2010/WMT2010_L1forSMTReranking.ps}
}

@INPROCEEDINGS{yuret-bicici:2009:Short,
   ADDRESS      = {Suntec, Singapore},
   AUTHOR       = {Deniz Yuret and Ergun Bicici},
   BOOKTITLE    = {Proceedings of the ACL-IJCNLP 2009 Conference Short Papers},
   OPTEDITOR    = {},
   MONTH        = {August},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {345--348},
   PUBLISHER    = {Association for Computational Linguistics},
   OPTSERIES    = {},
   TITLE        = {Modeling Morphologically Rich Languages Using Split 
      Words and Unstructured Dependencies},
   OPTVOLUME    = {},
   YEAR         = {2009},
   ABSTRACT     = {We experiment with splitting words into their stem and 
      suffix components for modeling morphologically rich languages. We 
      show that using a morphological analyzer and disambiguator results in 
      a significant perplexity reduction in Turkish. We present flexible 
      n-gram models, FlexGrams, which assume that the n-1 tokens that 
      determine the probability of a given token can be chosen anywhere in 
      the sentence rather than the preceding n-1 positions. Our final model 
      achieves 27\% perplexity reduction compared to the standard n-gram 
      model.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Language Modeling},
   URL          = {http://www.aclweb.org/anthology/P/P09/P09-2087},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2009/MMRLACL.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2009/MMRLACL.ps}
}

@ARTICLE{Bicici:JMAGS07,
   AUTHOR       = {Ergun Biçici},
   JOURNAL      = {Journal of Multiagent and Grid Systems},
   OPTMONTH     = {},
   OPTNOTE      = {},
   NUMBER       = {3},
   PAGES        = {297-314},
   TITLE        = {Consensus Ontologies in Socially Interacting MultiAgent 
      Systems},
   VOLUME       = {4},
   YEAR         = {2008},
   ABSTRACT     = {This paper presents approaches for building, managing, 
      and evaluating consensus ontologies from the individual ontologies of 
      a network of socially interacting agents. Each agent has its own 
      conceptualization of the world within the multiagent system 
      framework. The interactions between agents are modeled by sending 
      queries and receiving responses and later assessing each other's 
      performance based on the results. This model enables us to measure 
      the \emph{quality} of the societal beliefs in the resources which we 
      represent as the \emph{expertise} in each domain. The dynamic nature 
      of our system allows us to model the emergence of consensus that 
      mimics the evolution of language. We present an algorithm for 
      generating the consensus ontologies which makes use of the 
      authoritative agent's conceptualization in a given domain. As the 
      expertise of agents changes after a number of interactions, the 
      consensus ontology that we build based on the agents' individual 
      views evolves. The resulting approach is concordant with the 
      principles of emergent semantics. We provide formal definitions for 
      the problem of finding a consensus ontology in a step by step manner. 
      We evaluate the consensus ontologies by using different heuristic 
      measures of similarity based on the component ontologies. Conceptual 
      processing methods for generating, manipulating, and evaluating 
      consensus ontologies are given and experimental results are 
      presented. The presented approach looks promising and opens new 
      directions for further research.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Consensus Ontology},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2007/JMAGS.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2007/JMAGS.ps}
}

@INPROCEEDINGS{Bicici:CICLing08,
   ADDRESS      = {Haifa, Israel},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2008), LNCS},
   OPTEDITOR    = {},
   JOURNAL      = {Lecture Notes in Computer Science},
   MONTH        = {February},
   NOTE         = {&copy Springer-Verlag},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Context-Based Sentence Alignment in Parallel Corpora},
   OPTVOLUME    = {},
   YEAR         = {2008},
   ABSTRACT     = {This paper presents a language-independent context-based 
      sentence alignment technique given parallel corpora. We can view the 
      problem of aligning sentences as finding translations of sentences 
      chosen from different sources. Unlike current approaches which rely 
      on pre-defined features and models, our algorithm employs features 
      derived from the distributional properties of words and does not use 
      any language dependent knowledge. We make use of the context of 
      sentences and the notion of Zipfian word vectors which effectively 
      models the distributional properties of words in a given sentence. We 
      accept the context to be the frame in which the reasoning about 
      sentence alignment is done. We evaluate the performance of our system 
      based on two different measures: sentence alignment accuracy and 
      sentence alignment coverage. We compare the performance of our system 
      with commonly used sentence alignment systems and show that our 
      system performs 1.2149 to 1.6022 times better in reducing the error 
      rate in alignment accuracy and coverage for moderately sized corpora.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Artificial Intelligence, Machine Translation},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2008/ContextBasedSAPC.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2008/ContextBasedSAPC.ps}
}

@INPROCEEDINGS{BiciciDymetman:CICLing08,
   ADDRESS      = {Haifa, Israel},
   AUTHOR       = {Ergun Biçici and Marc Dymetman},
   BOOKTITLE    = {Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2008), LNCS},
   OPTEDITOR    = {},
   JOURNAL      = {Lecture Notes in Computer Science},
   MONTH        = {February},
   NOTE         = {&copy Springer-Verlag},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Dynamic Translation Memory: Using Statistical Machine 
      Translation to improve Translation Memory Fuzzy Matches},
   OPTVOLUME    = {},
   YEAR         = {2008},
   ABSTRACT     = {Professional translators of technical documents often 
      use Translation Memory (TM) systems in order to capitalize on the 
      repetitions frequently observed in these documents. TM systems 
      typically exploit not only complete matches between the source 
      sentence to be translated and some previously translated sentence, 
      but also so-called \emph{fuzzy matches}, where the source sentence 
      has some substantial commonality with a previously translated 
      sentence. These fuzzy matches can be very worthwhile as a starting 
      point for the human translator, but the translator then needs to 
      manually edit the associated TM-based translation to accommodate the 
      differences with the source sentence to be translated. If part of 
      this process could be automated, the cost of human translation could 
      be significantly reduced. The paper proposes to perform this 
      automation in the following way: a phrase-based Statistical Machine 
      Translation (SMT) system (trained on a bilingual corpus in the same 
      domain as the TM) is combined with the TM fuzzy match, by extracting 
      from the fuzzy-match a large (possibly gapped) bi-phrase that is 
      dynamically added to the usual set of ''static'' bi-phrases used for 
      decoding the source. We report experiments that show significant 
      improvements in terms of BLEU and NIST scores over both the 
      translations produced by the stand-alone SMT system and the 
      fuzzy-match translations proposed by the stand-alone TM system.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Artificial Intelligence, Machine Translation, 
      Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2008/DTM.pdf}
}

@PATENT{BiciciDymetmanDTM2008,
   ADDRESS      = {},
   AUTHOR       = {Ergun Biçici and Marc Dymetman},
   NOTE         = {US Patent 8,244,519},
   NUMBER       = {},
   TITLE        = {Dynamic translation memory using statistical machine 
      translation},
   YEAR         = {2008},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   OPTKEYWORDS  = {},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici:CONTEXT07,
   ADDRESS      = {Roskilde, Denmark},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {Proceedings of the 6th International and Interdisciplinary Conference on Modeling and Using Context (CONTEXT 2007), LNAI},
   OPTEDITOR    = {},
   JOURNAL      = {Lecture Notes in Artificial Intelligence},
   MONTH        = {August},
   NOTE         = {&copy Springer-Verlag},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {82--93},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Local Context Selection for Aligning Sentences in 
      Parallel Corpora},
   VOLUME       = {4635},
   YEAR         = {2007},
   ABSTRACT     = {This paper presents a novel language independent 
      context-based sentence alignment technique given parallel corpora. We 
      can view the problem of aligning sentences as finding translations of 
      sentences chosen from different sources. Unlike current approaches 
      which rely on pre-defined features and models, our algorithm employs 
      features derived from the distributional properties of sentences and 
      does not use any language dependent knowledge. We make use of the 
      context of sentences and introduce the notion of Zipfian word vectors 
      which effectively models the distributional properties of a given 
      sentence. We accept the context to be the frame in which the 
      reasoning about sentence alignment is done. We examine alternatives 
      for local context models and demonstrate that our context based 
      sentence alignment algorithm performs better than prominent sentence 
      alignment techniques. Our system dynamically selects the local 
      context for a pair of set of sentences which maximizes the 
      correlation. We evaluate the performance of our system based on two 
      different measures: sentence alignment accuracy and sentence 
      alignment coverage. We compare the performance of our system with 
      commonly used sentence alignment systems and show that our system 
      performs 1.1951 to 1.5404 times better in reducing the error rate in 
      alignment accuracy and coverage.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Artificial Intelligence, Machine Translation},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2007/ContextSelectioninLearningSAModels.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2007/ContextSelectioninLearningSAModels.ps}
}

@INPROCEEDINGS{Bicici:ICANNGA07,
   ADDRESS      = {Warsaw, Poland},
   AUTHOR       = {Ergun Biçici and Deniz Yuret},
   BOOKTITLE    = {Proceedings of the 8th International Conference on Adaptive and Natural Computing Algorithms (ICANNGA 2007), LNCS 4431},
   OPTEDITOR    = {},
   JOURNAL      = {Lecture Notes in Computer Science},
   MONTH        = {April},
   NOTE         = {&copy Springer-Verlag <br> <a href=../2007/LSDBC/ICANNGATalk.pdf>Presentation</a> <br> <a href=../2007/LSDBC/CLSDBC_README.txt>LSDBC README</a> <br> <a href=../2007/LSDBC/clsdbc>LSDBC Linux Executable</a> (For academic and research purposes) <br> <a href=../2007/LSDBC/clsdbcnl>LSDBC Linux Executable (no noise detection)</a> (For academic and research purposes)},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {739--748},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Locally Scaled Density Based Clustering},
   VOLUME       = {4431},
   YEAR         = {2007},
   ABSTRACT     = {Density based clustering methods allow the 
      identification of arbitrary, not necessarily convex regions of data 
      points that are densely populated. The number of clusters does not 
      need to be specified beforehand; a cluster is defined to be a 
      connected region that exceeds a given density threshold. This paper 
      introduces the notion of local scaling in density based clustering, 
      which determines the density threshold based on the local statistics 
      of the data. The local maxima of density are discovered using a 
      k-nearest-neighbor density estimation and used as centers of 
      potential clusters. Each cluster is grown until the density falls 
      below a pre-specified ratio of the center point's density. The 
      resulting clustering technique is able to identify clusters of 
      arbitrary shape on noisy backgrounds that contain significant density 
      gradients. The focus of this paper is to automate the process of 
      clustering by making use of the local density information for 
      arbitrarily sized, shaped, located, and numbered clusters. The 
      performance of the new algorithm is promising as it is demonstrated 
      on a number of synthetic datasets and images for a wide range of its 
      parameters.},
   OPTDOI       = {},
   ISBN         = {978-3-540-71589-4},
   OPTISSN      = {},
   KEYWORDS     = {Machine Learning},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2007/LSDBC/LSDBC-icannga07.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2007/LSDBC/LSDBC-icannga07.ps}
}

@INPROCEEDINGS{Bicici:COMPSAC2006,
   ADDRESS      = {Chicago, Illinois, USA},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {Proceedings of the 30th Annual International Computer Software and Applications Conference (COMPSAC'06)},
   OPTEDITOR    = {},
   MONTH        = {September},
   NOTE         = {&copy IEEE <br> <a href=../2006/COMPSACTalk.pdf>Presentation</a>},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {279--284},
   PUBLISHER    = {IEEE Computer Society},
   OPTSERIES    = {},
   TITLE        = {Consensus Ontology Generation in a Socially Interacting 
      MultiAgent System},
   OPTVOLUME    = {},
   YEAR         = {2006},
   ABSTRACT     = {This paper presents an approach for building consensus 
      ontologies from the individual ontologies of a network of socially 
      interacting agents. Each agent has its own conceptualization of the 
      world. The interactions between agents are modeled by sending queries 
      and receiving responses and later assessing each otherâs 
      performance based on the results. This model enables us to measure 
      the quality of the societal beliefs in the resources which we 
      represent as the expertise in each domain. The dynamic nature of our 
      system allows us to model the emergence of consensus that mimics the 
      evolution of language. We present an algorithm for generating the 
      consensus ontologies which makes use of the authoritative agentâs 
      conceptualization in a given domain. As the expertise of agents 
      change after a number of interactions, the consensus ontology that we 
      build based on the agentsâ individual views evolves. We evaluate 
      the consensus ontologies by using different heuristic measures of 
      similarity based on the component ontologies.},
   DOI          = {10.1109/COMPSAC.2006.126},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Consensus Ontology},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2006/COMPSAC2006Extended.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2006/COMPSAC2006Extended.ps}
}

@INPROCEEDINGS{Bicici:AMTA2006,
   ADDRESS      = {Québec City, Québec, Canada},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {Proceedings of the International Workshop on Agents and Multiagent Systems, from Theory to Application (AMTA 2006)},
   OPTEDITOR    = {},
   MONTH        = {June},
   OPTNOTE      = {},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Generating Consensus Ontologies among Socially 
      Interacting Agents},
   TYPE         = {workshop},
   OPTVOLUME    = {},
   YEAR         = {2006},
   ABSTRACT     = {This paper presents an approach for building consensus 
      ontologies from the individual ontologies of a network of socially 
      interacting agents. Each agent has its own conceptualization of the 
      world. The interactions between agents are modeled by sending queries 
      and receiving responses and later assessing each otherâs 
      performance based on the results. This model enables us to measure 
      the quality of the societal beliefs in the resources which we 
      represent as the expertise in each domain. The dynamic nature of our 
      system allows us to model the emergence of consensus that mimics the 
      evolution of language. We present an algorithm for generating the 
      consensus ontologies which makes use of the authoritative agentâs 
      conceptualization in a given domain. As the expertise of agents 
      change after a number of interactions, the consensus ontology that we 
      build based on the agentsâ individual views evolves. We evaluate 
      the consensus ontologies by using different heuristic measures of 
      similarity based on the component ontologies.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Consensus Ontology},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2006/AMTA2006Extended.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2006/AMTA2006Extended.ps}
}

@INPROCEEDINGS{BiciciTAINN06,
   ADDRESS      = {Akyaka, Mugla},
   AUTHOR       = {Ergun Biçici and Deniz Yuret},
   BOOKTITLE    = {Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN '06)},
   OPTEDITOR    = {},
   MONTH        = {June},
   NOTE         = {<a href=../2006/TAINNTalk.pdf>Presentation</a>},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   PAGES        = {277--284},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Clustering Word Pairs to Answer Analogy Questions},
   OPTVOLUME    = {},
   YEAR         = {2006},
   ABSTRACT     = {We focus on answering word analogy questions by using 
      clustering techniques. The increased performance in answering word 
      similarity questions can have many possible applications, including 
      question answering and information retrieval. We present an analysis 
      of clustering algorithmsâ performance on answering word similarity 
      questions. This paperâs contributions can be summarized as: (i) 
      casting the problem of solving word analogy questions as an instance 
      of learning clusterings of data and measuring the effectiveness of 
      prominent clustering techniques in learning semantic relations; (ii) 
      devising a heuristic approach to combine the results of different 
      clusterings for the purpose of distinctly separating word pair 
      semantics; (iii) answering SAT-type word similarity questions using 
      our technique.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2006/LAWSQ-LNCS.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2006/LAWSQ-LNCS.ps}
}

@TECHREPORT{,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   INSTITUTION  = {},
   OPTMONTH     = {},
   NOTE         = {Technical Report},
   OPTNUMBER    = {},
   TITLE        = {Formal Consensus},
   TYPE         = {Research Note},
   YEAR         = {2005},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Consensus Ontology},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

@INPROCEEDINGS{Bicici:DrugAER,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici},
   BOOKTITLE    = {North Carolina Symposium on Biotechnology & Bioinformatics, IEEE Tech4Life},
   OPTEDITOR    = {},
   OPTMONTH     = {},
   NOTE         = {<a href=../2004/IEEETech4LifePresentation.pdf>Presentation</a>},
   OPTNUMBER    = {},
   OPTORGANIZATION = {},
   OPTPAGES     = {},
   OPTPUBLISHER = {},
   OPTSERIES    = {},
   TITLE        = {Relational Learning from Drug Adverse Events Reports},
   TYPE         = {workshop},
   OPTVOLUME    = {},
   YEAR         = {2004},
   ABSTRACT     = {We applied relational learning to discover rules from 
      adverse events reports. We used the FOIL relational learning system 
      to Â¯nd a set of rules for withdrawn drugs. We compared our results 
      with FDA's reasons for withdrawal.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Inductive Logic Programming, Machine Learning},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2004/IEEETech4LifeConf2004.pdf}
}

@TECHREPORT{Bicici:RAFTA,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici and Robert St. Amant},
   INSTITUTION  = {North Carolina State University},
   OPTMONTH     = {},
   NOTE         = {<a href=../2003/>Presentation</a>},
   NUMBER       = {TR-2003-22},
   TITLE        = {Reasoning About the Functionality of Tools and Physical 
      Artifacts},
   OPTTYPE      = {},
   YEAR         = {2003},
   ABSTRACT     = {Tool use is an important characteristic of intelligent 
      human behavior. Representing, classifying and recognizing tools by 
      their functionality can provide us new opportunities for 
      understanding and eventually improving an agent's interaction with 
      the physical world. Techniques have been developed in a wide range of 
      areas within artificial intelligence and other disciplines to 
      represent and automatically reason about the functionality of tools. 
      This article surveys past approaches to reasoning about functionality 
      in the literature and attempts to give an overview of the strengths 
      and weaknesses of previous techniques. A number of issues that needs 
      to be addressed are also reviewed.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Artificial Intelligence},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2003/ebiciciRAFTPA.pdf},
   PS           = {http://www.computing.dcu.ie/~ebicici/publications/2003/ebiciciRAFTPA.ps}
}

@UNPUBLISHED{IUIT2003,
   AUTHOR       = {Sameer Rajyaguru and Ergun Biçici and Robert St. Amant},
   OPTMONTH     = {},
   NOTE         = {MSc Work},
   TITLE        = {Intelligent User Interface Tools},
   YEAR         = {2003},
   ABSTRACT     = {This paper describes an analysis tool for dynamic gaming 
      environments. It is based on an image processing system and a 
      contingency analysis tool for extracting the rules that the user is 
      applying in a driving game simulation. The tool is intended for a 
      cognitive modelling architecture that will learn from the user's 
      previous actions to minimize the amount of assumptions made to 
      control the environment. Interaction with the dynamically changing 
      visual environments at real-time is important. This paper discusses 
      our initial attempt to analyze dynamic gaming environments through 
      statistical measures, our learning goal from previous experiences, 
      the limitations, and its implications for user interface environments 
      that dynamically change.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Artificial Intelligence},
   OPTURL       = {},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2003/ebiciciIDA2003.pdf}
}

@ARTICLE{ProlegomenonCSRUI,
   AUTHOR       = {Ergun Biçici},
   JOURNAL      = {ACM Crossroads},
   OPTMONTH     = {},
   OPTNOTE      = {},
   NUMBER       = {1},
   OPTPAGES     = {},
   SERIES       = {Human Computer Interaction},
   TITLE        = {Prolegomenon to Commonsense Reasoning in User Interfaces},
   VOLUME       = {9},
   YEAR         = {2002},
   ABSTRACT     = {Human-computer interaction experiences unbalanced 
      talents of counterparts in the user interface, which can only be 
      eased with the introduction of new solutions. Commonsense reasoning 
      is a promising answer that offers formalization and computational 
      models about how humans reason and think in a sensible way. In user 
      interface design, assumed conventions and rules are widely followed 
      and carried to user interfaces. For the most part, these assumptions 
      are obvious to humans yet incomprehensible to computers. As a result, 
      it is essential that tools are developed, capable of retrieving 
      relevant, sensible inferences that in turn can serve as catalysts for 
      future reasoning. Embedding this tool in user interfaces can provide 
      many benefits including representation of assumptions and unspoken 
      rules, the addition of useful tool abilities, and increasingly usable 
      and accessible environments where computers and humans have extended 
      communication capabilities to assist in understanding each other. In 
      this article, we present groundwork for applying commonsense 
      reasoning to user interfaces. We start with identifying the asymmetry 
      in human-machine communication and later focus on some approaches 
      such as softbots and the proposed anti-mac interface. Then we 
      identify the problems faced in user interfaces such as the 
      correspondence problem inherited from computer vision. Next, we state 
      our research ambition as adding commonsense reasoning functionality 
      to user interfaces and further survey previous approaches and the 
      state of the art. We depict the big picture we are facing and list 
      some of the rewards to be earned by applying this technique. Later, 
      characteristics of common sense are investigated together with 
      examples in first-order logic. We then report various lessons learned 
      from earlier attempts that began in physical systems concentrated on 
      small microworld problems. Finally, we cite a sample methodology for 
      automating commonsense reasoning and identify a number of questions 
      that have yet to be answered.},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Artificial Intelligence},
   URL          = {http://www.computing.dcu.ie/~ebicici/publications/2002/ACMCrossroads2002/CommonsenseUI6.htm},
   OPTURL-PUBLISHER = {},
   PDF          = {http://www.computing.dcu.ie/~ebicici/publications/2002/ACMCrossroads2002/Prolegomenon2.pdf}
}

@TECHREPORT{COANTT2000,
   OPTADDRESS   = {},
   AUTHOR       = {Ergun Biçici and Baris Arslan},
   INSTITUTION  = {Bilkent University},
   OPTMONTH     = {},
   NOTE         = {Senior Project under the supervision of Kemal Oflazer},
   OPTNUMBER    = {},
   TITLE        = {Corpus Annotation Tool for Turkish Language},
   OPTTYPE      = {},
   YEAR         = {2000},
   OPTABSTRACT  = {},
   OPTDOI       = {},
   OPTISBN      = {},
   OPTISSN      = {},
   KEYWORDS     = {Natural Language Processing},
   OPTURL       = {},
   OPTURL-PUBLISHER = {}
}

